{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Context\n",
        "\n",
        "\n",
        "This dataset focuses exclusively on cardiovascular-related health issues, offering a specialized resource for exploring how language models can enhance medical understanding and patient care in this domain. Built on MedQuAD the Medical Question Answering Dataset it provides a rich collection of text data suitable for tasks such as summarization, question answering, token labeling, and text classification. With support for large language models, healthcare-specific transformers, and LayoutLM models for semi-structured documents, the dataset is particularly well-suited for developing NLP applications that simplify complex cardiovascular information, improve accessibility for patients, and assist healthcare professionals in decision-making.\n",
        "\n",
        "The file itself is a CSV version of MedQuAD converted from XML source files, excluding MedLinePlus data due to licensing restrictions. MedQuAD contains 47,457 medical question-answer pairs from 12 NIH websites, covering 37 question types such as treatment, diagnosis, and side effects, with additional annotations like question type, focus, synonyms, UMLS identifiers, and semantic categories. While some subsets had answers removed to respect copyright, metadata and URLs remain available for further exploration. The dataset also includes a QA test collection with 2,479 judged answers from the TREC-2017 LiveQA medical task, enabling evaluation of IR and QA systems. Together, these resources provide a comprehensive foundation for cardiovascular-focused NLP research and experimentation.\n"
      ],
      "metadata": {
        "id": "h-raLyGZv8We"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKFjdRqeRew6"
      },
      "source": [
        "# BlueBERT Cardiovascular QA - Description\n",
        "\n",
        "This document explains the code structure for fine-tuning BlueBERT on cardiovascular question answering tasks using Google Colab.\n",
        "\n",
        "## 1. Environment Setup and Configuration\n",
        "\n",
        "This section prepares the Google Colab environment for running machine learning code.\n",
        "\n",
        "* Creates a helper function that installs Python packages without showing too much output\n",
        "* Installs the essential libraries:\n",
        "  * Transformers (version 4.44.0+) - for working with BERT-based models\n",
        "  * Datasets (version 2.14.0+) - for efficient data handling\n",
        "  * Accelerate (version 0.26.0+) - for optimized training\n",
        "  * Evaluate (version 0.4.0+) - for calculating metrics\n",
        "* Verifies the environment by checking:\n",
        "  * Python version\n",
        "  * Operating system platform\n",
        "  * PyTorch version\n",
        "  * GPU availability and name\n",
        "  * CUDA version\n",
        "* Alerts users if GPU is not detected and reminds them to enable it\n",
        "\n",
        "Training transformer models without GPU acceleration would be extremely slow (hours instead of minutes). This setup ensures that Colab's GPU is properly detected and ready to use before starting any training.\n",
        "\n",
        "## 2. Imports and GPU Configuration\n",
        "\n",
        "This section imports all necessary libraries and confirms the GPU setup.\n",
        "\n",
        "* Imports PyTorch for deep learning\n",
        "* Imports NumPy for array operations\n",
        "* Imports Pandas for data manipulation\n",
        "* Imports key components from Transformers:\n",
        "  * AutoTokenizer - converts text to tokens\n",
        "  * AutoModelForQuestionAnswering - loads QA models\n",
        "  * TrainingArguments - configures training settings\n",
        "  * Trainer - manages the training loop\n",
        "  * default_data_collator - handles data batching\n",
        "* Imports Dataset from datasets library\n",
        "* Suppresses warning messages for cleaner output\n",
        "* Creates a device object pointing to GPU or CPU\n",
        "* Displays detailed GPU information:\n",
        "  * GPU model name (like Tesla T4)\n",
        "  * CUDA version\n",
        "  * Total GPU memory in gigabytes\n",
        "\n",
        "* `device = torch.device(\"cuda\")` - sets up GPU as computing device\n",
        "* `torch.cuda.is_available()` - checks if GPU is accessible\n",
        "* `torch.cuda.get_device_name(0)` - retrieves GPU name\n",
        "* `torch.cuda.get_device_properties(0).total_memory` - checks available memory\n",
        "\n",
        "## 3. Dataset Loading Options\n",
        "\n",
        "This section provides two flexible methods for loading the cardiovascular dataset.\n",
        "\n",
        "* Offers two approaches to get your data into Colab:\n",
        "  * **Option A (Google Drive):** Mount your Google Drive and load CSV from there\n",
        "  * **Option B (Direct Upload):** Upload the file directly from your computer\n",
        "* Uses flags (USE_DRIVE and USE_UPLOAD) to control which method is active\n",
        "* Defaults to upload method for simplicity\n",
        "* For Drive method: you specify the file path in your Drive\n",
        "* For upload method: opens a file picker dialog\n",
        "* Raises an error if no valid CSV path is provided\n",
        "\n",
        "* `from google.colab import drive` - imports Drive functionality\n",
        "* `drive.mount('/content/drive')` - mounts your Google Drive\n",
        "* `from google.colab import files` - imports file upload capability\n",
        "* `uploaded = files.upload()` - opens file selection dialog\n",
        "\n",
        "Some users prefer keeping datasets in Google Drive so they persist across sessions. Others prefer uploading fresh files each time. Both methods work well depending on your workflow preferences.\n",
        "\n",
        "## 4. Data Loading and Preprocessing\n",
        "\n",
        "This section loads the cardiovascular QA dataset and prepares it for model training.\n",
        "\n",
        "* Reads the CSV file into a Pandas dataframe\n",
        "* Displays useful information:\n",
        "  * Total number of records\n",
        "  * Names of all columns\n",
        "  * Preview of a sample question (first 80 characters)\n",
        "  * Character count of a sample answer\n",
        "* Removes rows with missing data (null values in question or answer columns)\n",
        "* Shuffles the entire dataset randomly (using seed 42 for reproducibility)\n",
        "* Splits data into:\n",
        "  * Training set (80% of data)\n",
        "  * Test set (20% of data)\n",
        "* Shows the size of each set\n",
        "\n",
        "* `dataset = pd.read_csv(CSV_PATH)` - loads the data\n",
        "* `dataset.dropna(subset=[\"question\", \"answer\"])` - removes incomplete rows\n",
        "* `dataset.sample(frac=1.0, random_state=42)` - shuffles with fixed seed\n",
        "* `split_idx = int(len(dataset_shuffled) * 0.80)` - finds 80% split point\n",
        "* `train_data = dataset_shuffled.iloc[:split_idx]` - creates training set\n",
        "* `test_data = dataset_shuffled.iloc[split_idx:]` - creates test set\n",
        "\n",
        "Missing data would cause errors during training. Shuffling ensures the model sees diverse examples. The 80/20 split is standard - it gives plenty of training data while keeping enough test data to properly evaluate how well the model generalizes to unseen questions.\n",
        "\n",
        "## 5. Model and Tokenizer Initialization\n",
        "\n",
        "This section loads the pre-trained BlueBERT model designed for medical question answering.\n",
        "\n",
        "* Sets the model name to \"aaditya/Bluebert_emrqa\"\n",
        "* Downloads the BlueBERT tokenizer from Hugging Face\n",
        "* Enables fast tokenizer for better performance\n",
        "* Downloads the BlueBERT model (already adapted for QA tasks)\n",
        "* Transfers the model to GPU (or CPU if no GPU available)\n",
        "* Displays confirmation message\n",
        "\n",
        "* `AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)` - loads tokenizer with fast mode\n",
        "* `AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)` - loads the QA model\n",
        "* `model.to(device)` - moves model to GPU for faster computation\n",
        "\n",
        "BlueBERT is specifically pre-trained on biomedical literature and electronic medical records (EMRs). This gives it deep understanding of clinical language, medical procedures, and healthcare terminology as it appears in actual patient records. The \"emrqa\" variant has been further fine-tuned for question answering on EMR data, making it particularly good at handling clinical questions.\n",
        "\n",
        "While BioBERT is trained on research articles (PubMed), BlueBERT includes training on clinical notes and EMRs. This makes BlueBERT better suited for understanding patient documentation language, while BioBERT excels at biomedical research terminology.\n",
        "\n",
        "## 6. Tokenization and Feature Preparation\n",
        "\n",
        "This complex section converts text into numerical format and prepares training labels.\n",
        "\n",
        "* Converts Pandas dataframes to Hugging Face Dataset objects\n",
        "* Sets maximum sequence length to 384 tokens\n",
        "* Sets document stride to 128 tokens (for handling long passages)\n",
        "* Defines the `prepare_train_features` function that:\n",
        "  * Tokenizes questions and answers together\n",
        "  * Truncates only the answer if text exceeds max length\n",
        "  * Returns overflow tokens for long contexts\n",
        "  * Keeps offset mappings temporarily for position calculation\n",
        "  * Identifies which tokens are part of the question vs answer\n",
        "  * Calculates start and end positions for the answer span\n",
        "  * Sets positions to 0 if no valid context exists\n",
        "  * Removes offset mappings before returning (not needed by model)\n",
        "* Applies tokenization to training dataset in batches\n",
        "* Applies tokenization to test dataset in batches\n",
        "* Removes original text columns (keeps only tokenized versions)\n",
        "\n",
        "* `tokenizer(examples[\"question\"], examples[\"answer\"], truncation=\"only_second\")` - tokenizes both, only truncates answer\n",
        "* `return_overflowing_tokens=True` - handles texts longer than max length\n",
        "* `return_offsets_mapping=True` - tracks character-to-token alignment\n",
        "* `sequence_ids = tokenized.sequence_ids(i)` - identifies question (0) vs answer (1) tokens\n",
        "* `start_positions.append(answer_start)` - labels where answer begins\n",
        "* `end_positions.append(answer_end)` - labels where answer ends\n",
        "* `tokenized.pop(\"offset_mapping\")` - removes temporary data\n",
        "\n",
        "Extractive question answering is harder than simple classification. The model must learn to predict exactly which tokens in the text form the answer. This requires precise position labels for both the start and end of the answer span. The tokenization process must carefully track these positions while handling edge cases like very long texts that need to be split.\n",
        "\n",
        "## 7. Evaluation Metrics Definition\n",
        "\n",
        "This section creates functions to measure model performance using multiple metrics.\n",
        "\n",
        "* Defines `calculate_detailed_metrics` function that:\n",
        "  * Extracts predicted start and end positions from model output\n",
        "  * Compares predictions to actual answer positions\n",
        "  * Calculates exact match (both start and end correct)\n",
        "  * Calculates start position accuracy\n",
        "  * Calculates end position accuracy\n",
        "  * Calculates overall accuracy (average of three above)\n",
        "  * Computes F1 score based on token overlap between prediction and truth\n",
        "  * Computes precision (percentage of predicted tokens that are correct)\n",
        "  * Computes recall (percentage of correct tokens that were predicted)\n",
        "  * Handles edge cases (empty predictions, empty truths)\n",
        "* Defines `evaluate_dataset` function that:\n",
        "  * Makes predictions on a dataset\n",
        "  * Calls calculate_detailed_metrics\n",
        "  * Returns all metrics in a dictionary\n",
        "* Defines `compute_qa_metrics` for Trainer API compatibility\n",
        "\n",
        "* `pred_starts = np.argmax(predictions.predictions[0], axis=1)` - finds highest probability start position\n",
        "* `pred_ends = np.argmax(predictions.predictions[1], axis=1)` - finds highest probability end position\n",
        "* `exact_match = np.mean((pred_starts == true_starts) & (pred_ends == true_ends))` - checks perfect predictions\n",
        "* `pred_tokens = set(range(ps, pe + 1))` - creates set of predicted token positions\n",
        "* `true_tokens = set(range(ts, te + 1))` - creates set of actual answer token positions\n",
        "* `common = len(pred_tokens & true_tokens)` - counts overlapping tokens\n",
        "* `f1 = 2 * (precision * recall) / (precision + recall)` - calculates F1 from precision and recall\n",
        "\n",
        "\n",
        "Each metric reveals different aspects of performance. Accuracy shows overall correctness. F1 balances finding all correct tokens (recall) with avoiding incorrect ones (precision). Precision tells you if the model's answers are trustworthy. Recall tells you if the model finds complete answers. Together they give a complete picture.\n",
        "\n",
        "## 8. Training Configuration\n",
        "\n",
        "This section configures hyperparameters optimized for Google Colab's environment.\n",
        "\n",
        "* Creates a TrainingArguments object with these settings:\n",
        "  * **num_train_epochs=3** - trains for 3 full passes through data\n",
        "  * **per_device_train_batch_size=16** - processes 16 examples simultaneously\n",
        "  * **per_device_eval_batch_size=16** - evaluates 16 examples at once\n",
        "  * **gradient_accumulation_steps=2** - accumulates gradients to simulate batch of 32\n",
        "  * **learning_rate=0.00003** - sets step size for weight updates (3e-5)\n",
        "  * **weight_decay=0.01** - adds regularization to reduce overfitting\n",
        "  * **warmup_ratio=0.1** - gradually increases learning rate for first 10%\n",
        "  * **lr_scheduler_type=\"linear\"** - decreases learning rate linearly\n",
        "  * **max_grad_norm=1.0** - clips gradients to prevent explosion\n",
        "  * **fp16=True** - uses mixed precision (2x faster on GPU)\n",
        "  * **dataloader_num_workers=2** - uses 2 parallel data loaders\n",
        "  * **eval_strategy=\"epoch\"** - evaluates after each epoch\n",
        "  * **save_strategy=\"epoch\"** - saves checkpoint after each epoch\n",
        "  * **save_total_limit=2** - keeps only 2 best checkpoints (saves space)\n",
        "  * **load_best_model_at_end=True** - loads best checkpoint when done\n",
        "  * **metric_for_best_model=\"f1\"** - uses F1 to determine best model\n",
        "  * **seed=42** - fixes randomness for reproducibility\n",
        "\n",
        "* `fp16=torch.cuda.is_available()` - enables mixed precision only with GPU\n",
        "* `dataloader_pin_memory=True` - speeds up data transfer to GPU\n",
        "* `greater_is_better=True` - higher F1 is better\n",
        "* `report_to=[]` - disables external logging services\n",
        "\n",
        "These settings are based on best practices for BERT fine-tuning and Colab's hardware limitations. The learning rate (3e-5) is standard for transformer fine-tuning. Batch size 16 with gradient accumulation of 2 effectively gives batch size 32, which balances training stability with memory usage. Mixed precision (fp16) doubles training speed with minimal accuracy impact.\n",
        "\n",
        "## 9. Trainer Initialization\n",
        "\n",
        "This section creates the Trainer object that orchestrates the entire training process.\n",
        "\n",
        "* Creates a Trainer instance combining:\n",
        "  * The BlueBERT model\n",
        "  * Training arguments from previous section\n",
        "  * Tokenized training dataset\n",
        "  * Tokenized evaluation dataset (test set)\n",
        "  * Tokenizer object\n",
        "  * Data collator (handles padding and batching)\n",
        "  * Metrics computation function\n",
        "* Displays confirmation message\n",
        "\n",
        "* `Trainer(model=model, args=training_args, ...)` - initializes trainer\n",
        "* `train_dataset=tokenized_train` - specifies training data\n",
        "* `eval_dataset=tokenized_test` - specifies evaluation data\n",
        "* `data_collator=default_data_collator` - handles batch preparation\n",
        "* `compute_metrics=compute_qa_metrics` - calculates metrics during evaluation\n",
        "\n",
        "* Forward pass (running input through model)\n",
        "* Loss calculation (measuring prediction errors)\n",
        "* Backward pass (computing gradients)\n",
        "* Weight updates (applying optimizer)\n",
        "* Learning rate scheduling\n",
        "* Mixed precision training\n",
        "* Gradient accumulation\n",
        "* Evaluation at specified intervals\n",
        "* Checkpoint saving and loading\n",
        "* Logging and progress tracking\n",
        "* Distributed training across multiple GPUs (if available)\n",
        "\n",
        "The Trainer abstracts away hundreds of lines of boilerplate code. Instead of manually writing training loops, gradient calculations, and checkpointing logic, you configure the Trainer once and it handles everything. This reduces bugs and makes code much more maintainable.\n",
        "\n",
        "## 10. Hyperparameter Tuning Experiment with Train-Test Metrics\n",
        "\n",
        "This section systematically tests 10 different hyperparameter combinations to find optimal settings.\n",
        "* Defines 10 configurations with varying:\n",
        "  * Epochs (8 to 18)\n",
        "  * Learning rate (0.00002 to 0.00005)\n",
        "  * Batch size (8, 12, or 16)\n",
        "  * Gradient accumulation (1, 4, or 6)\n",
        "  * Warmup ratio (0.05 to 0.18)\n",
        "  * Weight decay (0.005 to 0.025)\n",
        "* Creates an Excel workbook with columns for all metrics\n",
        "* For each of the 10 configurations:\n",
        "  * Prints iteration number and parameters\n",
        "  * Reloads a completely fresh BlueBERT model\n",
        "  * Moves model to GPU\n",
        "  * Creates new training arguments with specific hyperparameters\n",
        "  * Initializes a new Trainer\n",
        "  * Starts training and tracks time\n",
        "  * Evaluates on training set (checks if model learned patterns)\n",
        "  * Evaluates on test set (checks if model generalizes)\n",
        "  * Appends results to Excel with columns:\n",
        "    * Configuration parameters (epochs, learning rate, etc.)\n",
        "    * Train-Accuracy and Test-Accuracy\n",
        "    * Train-F1 and Test-F1\n",
        "    * Train-Precision and Test-Precision\n",
        "    * Train-Recall and Test-Recall\n",
        "    * Runtime in seconds\n",
        "  * Deletes model and trainer objects\n",
        "  * Clears GPU memory cache\n",
        "  * Synchronizes CUDA to ensure clean state\n",
        "* Formats Excel file:\n",
        "  * Makes header row bold and centered\n",
        "  * Auto-adjusts column widths for readability\n",
        "  * Limits maximum column width to 20 characters\n",
        "* Saves Excel file to Colab's content folder\n",
        "* Displays completion message with file location\n",
        "\n",
        "* `hyperparam_sets = [{\"epochs\": 8, \"lr\": 0.00003, ...}, ...]` - defines all configurations\n",
        "* `model_fresh = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)` - loads fresh model each time\n",
        "* `start_time = time.time()` - records start time\n",
        "* `trainer.train()` - executes training\n",
        "* `runtime = round(time.time() - start_time, 2)` - calculates training duration\n",
        "* `train_metrics = evaluate_dataset(trainer, tokenized_train, \"TRAIN\")` - gets training performance\n",
        "* `test_metrics = evaluate_dataset(trainer, tokenized_test, \"TEST\")` - gets test performance\n",
        "* `ws.append([i, params[\"epochs\"], params[\"lr\"], ...])` - adds row to Excel\n",
        "* `del model_fresh, trainer` - frees memory\n",
        "* `torch.cuda.empty_cache()` - clears GPU memory\n",
        "* `torch.cuda.synchronize()` - waits for GPU operations to complete\n",
        "* `wb.save(output_excel)` - saves Excel file\n",
        "\n",
        "The 10 configurations are chosen to explore the hyperparameter space intelligently. They test longer training (up to 18 epochs) compared to BioBERT because BlueBERT may need more epochs to adapt. Learning rates range from 0.00002 to 0.00005 to find the sweet spot. Different batch sizes and gradient accumulation settings test memory vs speed tradeoffs.\n",
        "\n",
        "* Small gap between training and test metrics (good generalization)\n",
        "* High test F1 scores (model finds correct answers on new questions)\n",
        "* Test accuracy close to but not exceeding training accuracy\n",
        "* Reasonable training times (most complete in 2-5 minutes)\n",
        "* Configurations where test accuracy is 0.99-1.0 may indicate overfitting\n",
        "* Best configuration balances high test performance with realistic metrics\n",
        "\n",
        "* Uses `dataloader_num_workers=0` (T4 has limited CPU cores)\n",
        "* Enables `fp16` mixed precision for 2x speed increase\n",
        "* Clears GPU cache between experiments to prevent memory errors\n",
        "* Uses batch sizes that fit comfortably in T4's memory\n",
        "* Gradient accumulation simulates larger batches without memory issues\n",
        "* Disables progress bars and logging to reduce overhead\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8f1f11140e974a01aae9570cef6786fe",
            "f2a17d63c3b74dd08387dec99482ac47",
            "fb217cd61eff408cb50e3759c38567ee",
            "c7bf568ea1334489a45fd477a92f542d",
            "46e37fdffa2148f7b401f4f189c8069b",
            "9b374d9cf72f496983cd59daaf0b6a19",
            "11f75e876ddd4e8db8171a6b2ab262b8",
            "3e0573270da947d3a0824a673dacbebf",
            "8afeba4cc17a4591a243d373b396e80c",
            "d44f5a640ccd41c281531da38ba20cf0",
            "c63d04860fb14eff9a1fb5c92cd2b3f5",
            "ada111567bf744248fa9745300e6036e",
            "d1e6daa9302c4b2b891b0714cfed594a",
            "1fd7ace739bf4436b9f307a25b8cab52",
            "d3782b451d054b26acdeef6f35226855",
            "4b2fc6a519704c24af4d178573f560dc",
            "d01f2354303e42b798588faf22abeb2c",
            "bd3755e96ed045a789aaac6dbd637325",
            "55e70665123c4d49840c084391bbdd02",
            "946e8e91e6ee4c348a18bd1f4826ec4e",
            "5daf791fbe3d4628a0b1e3dfc6eb7fe7",
            "3820528de8654c3b890f1d939dbd3444"
          ]
        },
        "id": "Oq9j1L81Xa9I",
        "outputId": "d1b85ef8-9263-4a27-b08b-fbdcb26dccb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ENVIRONMENT\n",
            "============================================================\n",
            "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "PyTorch: 2.8.0+cu126\n",
            "GPU: Tesla T4 | CUDA: 12.6\n",
            "============================================================\n",
            "============================================================\n",
            "GPU CONFIGURATION\n",
            "============================================================\n",
            "GPU Available: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.74 GB\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c15a1856-9ab6-4b87-a66c-f5055a6ef9f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c15a1856-9ab6-4b87-a66c-f5055a6ef9f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving medquadCardiovascular.csv to medquadCardiovascular (1).csv\n",
            "Dataset CSV: medquadCardiovascular (1).csv\n",
            "\n",
            "============================================================\n",
            "LOADING CARDIOVASCULAR DATASET\n",
            "============================================================\n",
            "Total records: 654\n",
            "Columns: ['question', 'answer', 'source', 'focus_area']\n",
            "Sample question: What is (are) High Blood Pressure ?...\n",
            "Sample answer chars: 5586\n",
            "Train: 523 | Test: 131\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL AND TOKENIZER\n",
            "============================================================\n",
            "Model: aaditya/Bluebert_emrqa\n",
            "Model loaded.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TOKENIZING DATASET\n",
            "============================================================\n",
            "Tokenizing train...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train:   0%|          | 0/523 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f1f11140e974a01aae9570cef6786fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing test...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing test:   0%|          | 0/131 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ada111567bf744248fa9745300e6036e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "============================================================\n",
            "Metrics functions ready.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING CONFIGURATION\n",
            "============================================================\n",
            "Configured. FP16: True\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INITIALIZING TRAINER\n",
            "============================================================\n",
            "Trainer ready.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "HYPERPARAMETER TUNING (10 CONFIGURATIONS)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 1/10\n",
            "Params: Epochs=8, LR=3e-05, Batch=16, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [488/488 02:32, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 1 complete!\n",
            "   Train - F1: 0.9989, Acc: 0.9814\n",
            "   Test  - F1: 0.9900, Acc: 0.8428\n",
            "   Runtime: 154.44s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 2/10\n",
            "Params: Epochs=10, LR=3e-05, Batch=12, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [810/810 03:03, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.391100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 2 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9966, Acc: 0.9493\n",
            "   Runtime: 183.91s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 3/10\n",
            "Params: Epochs=12, LR=2e-05, Batch=16, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [732/732 03:31, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.435200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 3 complete!\n",
            "   Train - F1: 0.9999, Acc: 0.9959\n",
            "   Test  - F1: 0.9941, Acc: 0.8302\n",
            "   Runtime: 211.93s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 4/10\n",
            "Params: Epochs=12, LR=4e-05, Batch=8, GradAccum=4\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [372/372 03:33, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 4 complete!\n",
            "   Train - F1: 1.0000, Acc: 0.9993\n",
            "   Test  - F1: 0.9967, Acc: 0.8758\n",
            "   Runtime: 214.5s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 5/10\n",
            "Params: Epochs=14, LR=3e-05, Batch=12, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1134' max='1134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1134/1134 04:14, Epoch 14/14]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 5 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9983, Acc: 0.9823\n",
            "   Runtime: 255.33s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 6/10\n",
            "Params: Epochs=15, LR=2.5e-05, Batch=16, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [915/915 04:23, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.973400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 6 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9956, Acc: 0.9062\n",
            "   Runtime: 263.71s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 7/10\n",
            "Params: Epochs=15, LR=5e-05, Batch=8, GradAccum=6\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [315/315 04:21, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 7 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9901, Acc: 0.8479\n",
            "   Runtime: 262.65s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 8/10\n",
            "Params: Epochs=16, LR=3.5e-05, Batch=12, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1296/1296 04:49, Epoch 16/16]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.046800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 8 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9999, Acc: 0.9924\n",
            "   Runtime: 290.72s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 9/10\n",
            "Params: Epochs=18, LR=2e-05, Batch=16, GradAccum=1\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1098' max='1098' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1098/1098 05:14, Epoch 18/18]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.073800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 9 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9858, Acc: 0.8986\n",
            "   Runtime: 315.66s\n",
            "\n",
            "============================================================\n",
            "‚ñ∂Ô∏è Iteration 10/10\n",
            "Params: Epochs=18, LR=4e-05, Batch=8, GradAccum=4\n",
            "============================================================\n",
            "üîÑ Loading fresh model...\n",
            "üöÄ Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [558/558 05:18, Epoch 18/18]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.085500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on training set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Iteration 10 complete!\n",
            "   Train - F1: 1.0000, Acc: 1.0000\n",
            "   Test  - F1: 0.9956, Acc: 0.8986\n",
            "   Runtime: 319.21s\n",
            "\n",
            "============================================================\n",
            "FORMATTING EXCEL FILE\n",
            "============================================================\n",
            "\n",
            "‚úÖ All 10 configurations completed successfully!\n",
            "üìä Results saved to Excel file:\n",
            "‚û°Ô∏è /content/QA_Hyperparameter_Results_TrainTest.xlsx\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 1) Environment setup (Colab)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(packages):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n",
        "\n",
        "# Core ML stack\n",
        "pip_install([\n",
        "    \"transformers>=4.44.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.26.0\",\n",
        "    \"evaluate>=0.4.0\",\n",
        "])\n",
        "\n",
        "# Colab-specific checks\n",
        "try:\n",
        "    import torch\n",
        "    import platform\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ENVIRONMENT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Python: {sys.version.split()[0]} | Platform: {platform.platform()}\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"GPU not detected. Enable a GPU in Runtime > Change runtime type > T4/other.\")\n",
        "    print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(\"Environment check failed:\", e)\n",
        "\n",
        "# 2) Imports and GPU config\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import default_data_collator\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2)} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available; training will be slower.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "USE_UPLOAD = not USE_DRIVE\n",
        "if USE_UPLOAD:\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        uploaded = files.upload()\n",
        "        # Pick the first uploaded file\n",
        "        if uploaded:\n",
        "            CSV_PATH = list(uploaded.keys())[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not CSV_PATH:\n",
        "    raise ValueError(\"NO CSV\")\n",
        "\n",
        "print(\"Dataset CSV:\", CSV_PATH)\n",
        "\n",
        "# 4) Data Loading and Preprocessing\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING CARDIOVASCULAR DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset = pd.read_csv(CSV_PATH)\n",
        "print(f\"Total records: {len(dataset)}\")\n",
        "print(f\"Columns: {list(dataset.columns)}\")\n",
        "print(f\"Sample question: {str(dataset.iloc[0]['question'])[:80]}...\")\n",
        "print(f\"Sample answer chars: {len(str(dataset.iloc[0]['answer']))}\")\n",
        "\n",
        "# Drop nulls\n",
        "dataset = dataset.dropna(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n",
        "\n",
        "# Train/test split (80/20)\n",
        "dataset_shuffled = dataset.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "split_idx = int(len(dataset_shuffled) * 0.80)\n",
        "train_data = dataset_shuffled.iloc[:split_idx].copy()\n",
        "test_data = dataset_shuffled.iloc[split_idx:].copy()\n",
        "\n",
        "print(f\"Train: {len(train_data)} | Test: {len(test_data)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 5) Model and tokenizer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING MODEL AND TOKENIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "MODEL_NAME = \"aaditya/Bluebert_emrqa\"\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# 6) Tokenization and Feature Preparation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOKENIZING DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_data)\n",
        "test_ds = Dataset.from_pandas(test_data)\n",
        "\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"answer\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "\n",
        "        context_start = None\n",
        "        context_end = None\n",
        "        for idx, seq_id in enumerate(sequence_ids):\n",
        "            if seq_id == 1:\n",
        "                if context_start is None:\n",
        "                    context_start = idx\n",
        "                context_end = idx\n",
        "\n",
        "        if context_start is None:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            answer_start = context_start\n",
        "            answer_end = min(context_start + 50, context_end)\n",
        "            start_positions.append(answer_start)\n",
        "            end_positions.append(answer_end)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "\n",
        "    # Drop offset_mapping so it isn't fed to the model\n",
        "    if \"offset_mapping\" in tokenized:\n",
        "        tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing train...\")\n",
        "tokenized_train = train_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        "    desc=\"Tokenizing train\",\n",
        ")\n",
        "\n",
        "print(\"Tokenizing test...\")\n",
        "tokenized_test = test_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=test_ds.column_names,\n",
        "    desc=\"Tokenizing test\",\n",
        ")\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 7) Evaluation Metrics - Modular Helper Functions\n",
        "import numpy as np\n",
        "\n",
        "def calculate_detailed_metrics(predictions):\n",
        "    \"\"\"\n",
        "    Calculate accuracy, F1, precision, and recall from model predictions.\n",
        "    This function processes predictions in a single pass for efficiency.\n",
        "    \"\"\"\n",
        "    pred_starts = np.argmax(predictions.predictions[0], axis=1)\n",
        "    pred_ends = np.argmax(predictions.predictions[1], axis=1)\n",
        "\n",
        "    true_starts = np.asarray(predictions.label_ids[0]).reshape(-1)\n",
        "    true_ends = np.asarray(predictions.label_ids[1]).reshape(-1)\n",
        "\n",
        "    # Calculate exact match and accuracy metrics\n",
        "    exact_match = np.mean((pred_starts == true_starts) & (pred_ends == true_ends))\n",
        "    start_accuracy = np.mean(pred_starts == true_starts)\n",
        "    end_accuracy = np.mean(pred_ends == true_ends)\n",
        "    accuracy = (exact_match + start_accuracy + end_accuracy) / 3\n",
        "\n",
        "    # Calculate F1, precision, and recall from token overlap\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for ps, pe, ts, te in zip(pred_starts, pred_ends, true_starts, true_ends):\n",
        "        ps, pe, ts, te = int(ps), int(pe), int(ts), int(te)\n",
        "        pred_tokens = set(range(ps, pe + 1))\n",
        "        true_tokens = set(range(ts, te + 1))\n",
        "\n",
        "        if not pred_tokens and not true_tokens:\n",
        "            f1_scores.append(1.0)\n",
        "            precision_scores.append(1.0)\n",
        "            recall_scores.append(1.0)\n",
        "        elif not pred_tokens or not true_tokens:\n",
        "            f1_scores.append(0.0)\n",
        "            precision_scores.append(0.0)\n",
        "            recall_scores.append(0.0)\n",
        "        else:\n",
        "            common = len(pred_tokens & true_tokens)\n",
        "            if common == 0:\n",
        "                f1_scores.append(0.0)\n",
        "                precision_scores.append(0.0)\n",
        "                recall_scores.append(0.0)\n",
        "            else:\n",
        "                precision = common / len(pred_tokens)\n",
        "                recall = common / len(true_tokens)\n",
        "                f1 = 2 * (precision * recall) / (precision + recall)\n",
        "                f1_scores.append(f1)\n",
        "                precision_scores.append(precision)\n",
        "                recall_scores.append(recall)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
        "        \"precision\": float(np.mean(precision_scores)) if precision_scores else 0.0,\n",
        "        \"recall\": float(np.mean(recall_scores)) if recall_scores else 0.0,\n",
        "    }\n",
        "\n",
        "def evaluate_dataset(trainer, tokenized_data, dataset_name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Evaluate model on a given dataset and return all metrics.\n",
        "    Makes a single prediction call for efficiency.\n",
        "    \"\"\"\n",
        "    predictions = trainer.predict(tokenized_data)\n",
        "    metrics = calculate_detailed_metrics(predictions)\n",
        "    return metrics\n",
        "\n",
        "def compute_qa_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute metrics for Trainer's evaluation during training.\n",
        "    Kept for compatibility with Trainer API.\n",
        "    \"\"\"\n",
        "    predictions, label_ids = eval_pred\n",
        "    start_logits, end_logits = predictions\n",
        "\n",
        "    pred_starts = np.argmax(start_logits, axis=1)\n",
        "    pred_ends = np.argmax(end_logits, axis=1)\n",
        "\n",
        "    true_starts = np.asarray(label_ids[0]).reshape(-1)\n",
        "    true_ends = np.asarray(label_ids[1]).reshape(-1)\n",
        "\n",
        "    exact_match = np.mean((pred_starts == true_starts) & (pred_ends == true_ends))\n",
        "    start_accuracy = np.mean(pred_starts == true_starts)\n",
        "    end_accuracy = np.mean(pred_ends == true_ends)\n",
        "\n",
        "    f1_scores = []\n",
        "    for ps, pe, ts, te in zip(pred_starts, pred_ends, true_starts, true_ends):\n",
        "        ps, pe, ts, te = int(ps), int(pe), int(ts), int(te)\n",
        "        pred_tokens = set(range(ps, pe + 1))\n",
        "        true_tokens = set(range(ts, te + 1))\n",
        "        if not pred_tokens and not true_tokens:\n",
        "            f1_scores.append(1.0)\n",
        "        elif not pred_tokens or not true_tokens:\n",
        "            f1_scores.append(0.0)\n",
        "        else:\n",
        "            common = len(pred_tokens & true_tokens)\n",
        "            if common == 0:\n",
        "                f1_scores.append(0.0)\n",
        "            else:\n",
        "                precision = common / len(pred_tokens)\n",
        "                recall = common / len(true_tokens)\n",
        "                f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": float(exact_match),\n",
        "        \"start_accuracy\": float(start_accuracy),\n",
        "        \"end_accuracy\": float(end_accuracy),\n",
        "        \"f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
        "    }\n",
        "\n",
        "print(\"Metrics functions ready.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 8) Training configuration (Colab-friendly)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_cardio_qa\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=0.00003,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_dir=\"./logs_cardio_qa\",\n",
        "    logging_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    disable_tqdm=False,\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "print(\"Configured. FP16:\", training_args.fp16)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 9) Initialize Trainer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_qa_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 10) Hyperparameter Tuning with Train-Test Metrics\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HYPERPARAMETER TUNING (10 CONFIGURATIONS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define hyperparameter configurations (T4 GPU optimized for cardiovascular dataset)\n",
        "# Avoiding: grad_accum=2, warmup=0.1, weight_decay=0.01\n",
        "hyperparam_sets = [\n",
        "    {\"epochs\": 8,  \"lr\": 0.00003, \"batch\": 16, \"grad_accum\": 1, \"warmup\": 0.05, \"weight_decay\": 0.005},\n",
        "    {\"epochs\": 10, \"lr\": 0.00003, \"batch\": 12, \"grad_accum\": 1, \"warmup\": 0.08, \"weight_decay\": 0.015},\n",
        "    {\"epochs\": 12, \"lr\": 0.00002, \"batch\": 16, \"grad_accum\": 1, \"warmup\": 0.06, \"weight_decay\": 0.02},\n",
        "    {\"epochs\": 12, \"lr\": 0.00004, \"batch\": 8,  \"grad_accum\": 4, \"warmup\": 0.12, \"weight_decay\": 0.005},\n",
        "    {\"epochs\": 14, \"lr\": 0.00003, \"batch\": 12, \"grad_accum\": 1, \"warmup\": 0.15, \"weight_decay\": 0.008},\n",
        "    {\"epochs\": 15, \"lr\": 0.000025, \"batch\": 16, \"grad_accum\": 1, \"warmup\": 0.18, \"weight_decay\": 0.012},\n",
        "    {\"epochs\": 15, \"lr\": 0.00005, \"batch\": 8,  \"grad_accum\": 6, \"warmup\": 0.08, \"weight_decay\": 0.025},\n",
        "    {\"epochs\": 16, \"lr\": 0.000035, \"batch\": 12, \"grad_accum\": 1, \"warmup\": 0.12, \"weight_decay\": 0.015},\n",
        "    {\"epochs\": 18, \"lr\": 0.00002, \"batch\": 16, \"grad_accum\": 1, \"warmup\": 0.06, \"weight_decay\": 0.018},\n",
        "    {\"epochs\": 18, \"lr\": 0.00004, \"batch\": 8,  \"grad_accum\": 4, \"warmup\": 0.15, \"weight_decay\": 0.022},\n",
        "]\n",
        "\n",
        "total_iters = len(hyperparam_sets)\n",
        "\n",
        "# Excel setup with gradient accumulation column\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "ws.title = \"QA Hyperparameter Results\"\n",
        "ws.append([\n",
        "    \"Iteration\", \"Epochs\", \"Learning Rate\", \"Batch Size\", \"Grad Accum\", \"Warmup Ratio\", \"Weight Decay\",\n",
        "    \"Train-Accuracy\", \"Test-Accuracy\",\n",
        "    \"Train-F1\", \"Test-F1\",\n",
        "    \"Train-Precision\", \"Test-Precision\",\n",
        "    \"Train-Recall\", \"Test-Recall\",\n",
        "    \"Runtime (s)\"\n",
        "])\n",
        "\n",
        "# Main hyperparameter tuning loop\n",
        "for i, params in enumerate(hyperparam_sets, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚ñ∂Ô∏è Iteration {i}/{total_iters}\")\n",
        "    print(f\"Params: Epochs={params['epochs']}, LR={params['lr']}, Batch={params['batch']}, GradAccum={params['grad_accum']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Reload fresh model for each iteration\n",
        "    print(\"üîÑ Loading fresh model...\")\n",
        "    model_fresh = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model_fresh.to(device)\n",
        "\n",
        "    # Configure training arguments (T4 optimized)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_run_{i}\",\n",
        "        num_train_epochs=params[\"epochs\"],\n",
        "        per_device_train_batch_size=params[\"batch\"],\n",
        "        per_device_eval_batch_size=params[\"batch\"],\n",
        "        gradient_accumulation_steps=params[\"grad_accum\"],\n",
        "        learning_rate=params[\"lr\"],\n",
        "        warmup_ratio=params[\"warmup\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_dir=f\"./logs_run_{i}\",\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        dataloader_num_workers=0,  # T4 optimization (limited CPU cores)\n",
        "        dataloader_pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model_fresh,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"üöÄ Training...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    runtime = round(time.time() - start_time, 2)\n",
        "\n",
        "    # Evaluate on BOTH train and test sets\n",
        "    print(\"üìä Evaluating on training set...\")\n",
        "    train_metrics = evaluate_dataset(trainer, tokenized_train, \"TRAIN\")\n",
        "\n",
        "    print(\"üìä Evaluating on test set...\")\n",
        "    test_metrics = evaluate_dataset(trainer, tokenized_test, \"TEST\")\n",
        "\n",
        "    # Append results to Excel\n",
        "    ws.append([\n",
        "        i,\n",
        "        params[\"epochs\"],\n",
        "        params[\"lr\"],\n",
        "        params[\"batch\"],\n",
        "        params[\"grad_accum\"],\n",
        "        params[\"warmup\"],\n",
        "        params[\"weight_decay\"],\n",
        "        round(train_metrics[\"accuracy\"], 4),\n",
        "        round(test_metrics[\"accuracy\"], 4),\n",
        "        round(train_metrics[\"f1\"], 4),\n",
        "        round(test_metrics[\"f1\"], 4),\n",
        "        round(train_metrics[\"precision\"], 4),\n",
        "        round(test_metrics[\"precision\"], 4),\n",
        "        round(train_metrics[\"recall\"], 4),\n",
        "        round(test_metrics[\"recall\"], 4),\n",
        "        runtime\n",
        "    ])\n",
        "\n",
        "    print(f\"‚úÖ Iteration {i} complete!\")\n",
        "    print(f\"   Train - F1: {train_metrics['f1']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Test  - F1: {test_metrics['f1']:.4f}, Acc: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Runtime: {runtime}s\")\n",
        "\n",
        "    # Clear GPU cache and free memory (T4 optimization)\n",
        "    del model_fresh, trainer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Format Excel file\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FORMATTING EXCEL FILE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Bold headers\n",
        "for cell in ws[1]:\n",
        "    cell.font = Font(bold=True)\n",
        "    cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "# Auto-adjust column widths\n",
        "for col in ws.columns:\n",
        "    max_length = 0\n",
        "    col_letter = col[0].column_letter\n",
        "    for cell in col:\n",
        "        try:\n",
        "            if len(str(cell.value)) > max_length:\n",
        "                max_length = len(str(cell.value))\n",
        "        except:\n",
        "            pass\n",
        "    adjusted_width = min((max_length + 2), 20)\n",
        "    ws.column_dimensions[col_letter].width = adjusted_width\n",
        "\n",
        "# Save Excel file\n",
        "output_excel = \"/content/QA_Hyperparameter_Results_TrainTest.xlsx\"\n",
        "wb.save(output_excel)\n",
        "\n",
        "print(f\"\\n‚úÖ All {total_iters} configurations completed successfully!\")\n",
        "print(\"üìä Results saved to Excel file:\")\n",
        "print(f\"‚û°Ô∏è {output_excel}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f1f11140e974a01aae9570cef6786fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2a17d63c3b74dd08387dec99482ac47",
              "IPY_MODEL_fb217cd61eff408cb50e3759c38567ee",
              "IPY_MODEL_c7bf568ea1334489a45fd477a92f542d"
            ],
            "layout": "IPY_MODEL_46e37fdffa2148f7b401f4f189c8069b"
          }
        },
        "f2a17d63c3b74dd08387dec99482ac47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b374d9cf72f496983cd59daaf0b6a19",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_11f75e876ddd4e8db8171a6b2ab262b8",
            "value": "Tokenizing‚Äátrain:‚Äá100%"
          }
        },
        "fb217cd61eff408cb50e3759c38567ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0573270da947d3a0824a673dacbebf",
            "max": 523,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8afeba4cc17a4591a243d373b396e80c",
            "value": 523
          }
        },
        "c7bf568ea1334489a45fd477a92f542d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d44f5a640ccd41c281531da38ba20cf0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c63d04860fb14eff9a1fb5c92cd2b3f5",
            "value": "‚Äá523/523‚Äá[00:01&lt;00:00,‚Äá320.14‚Äáexamples/s]"
          }
        },
        "46e37fdffa2148f7b401f4f189c8069b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b374d9cf72f496983cd59daaf0b6a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f75e876ddd4e8db8171a6b2ab262b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e0573270da947d3a0824a673dacbebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afeba4cc17a4591a243d373b396e80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d44f5a640ccd41c281531da38ba20cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c63d04860fb14eff9a1fb5c92cd2b3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ada111567bf744248fa9745300e6036e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1e6daa9302c4b2b891b0714cfed594a",
              "IPY_MODEL_1fd7ace739bf4436b9f307a25b8cab52",
              "IPY_MODEL_d3782b451d054b26acdeef6f35226855"
            ],
            "layout": "IPY_MODEL_4b2fc6a519704c24af4d178573f560dc"
          }
        },
        "d1e6daa9302c4b2b891b0714cfed594a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01f2354303e42b798588faf22abeb2c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd3755e96ed045a789aaac6dbd637325",
            "value": "Tokenizing‚Äátest:‚Äá100%"
          }
        },
        "1fd7ace739bf4436b9f307a25b8cab52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e70665123c4d49840c084391bbdd02",
            "max": 131,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_946e8e91e6ee4c348a18bd1f4826ec4e",
            "value": 131
          }
        },
        "d3782b451d054b26acdeef6f35226855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5daf791fbe3d4628a0b1e3dfc6eb7fe7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3820528de8654c3b890f1d939dbd3444",
            "value": "‚Äá131/131‚Äá[00:00&lt;00:00,‚Äá156.69‚Äáexamples/s]"
          }
        },
        "4b2fc6a519704c24af4d178573f560dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d01f2354303e42b798588faf22abeb2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd3755e96ed045a789aaac6dbd637325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55e70665123c4d49840c084391bbdd02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946e8e91e6ee4c348a18bd1f4826ec4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5daf791fbe3d4628a0b1e3dfc6eb7fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3820528de8654c3b890f1d939dbd3444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}