{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94ce1350095344f59dc29be634067c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adcdfeee248445f9af7f7ee516aa8c3d",
              "IPY_MODEL_7f5016961ec446129e38a59aa28847c0",
              "IPY_MODEL_7fe52742f6da464e895618c810677387"
            ],
            "layout": "IPY_MODEL_acf3320043944a568059d6df442c975c"
          }
        },
        "adcdfeee248445f9af7f7ee516aa8c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c80ce414db0a4d7bab554b19c14b967a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc392b5aa16449c18749e3c33c643d7b",
            "value": "Tokenizingâ€‡train:â€‡100%"
          }
        },
        "7f5016961ec446129e38a59aa28847c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa9525b1e5064f4fb113cc0cf90130fa",
            "max": 555,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_955d4ef517294f54a545c1fb9b0a1dac",
            "value": 555
          }
        },
        "7fe52742f6da464e895618c810677387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b82d4cfb4bba42229ff6928d5c3a83d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27bbe6d56bb14607b579276b6df78b31",
            "value": "â€‡555/555â€‡[00:00&lt;00:00,â€‡613.05â€‡examples/s]"
          }
        },
        "acf3320043944a568059d6df442c975c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80ce414db0a4d7bab554b19c14b967a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc392b5aa16449c18749e3c33c643d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9525b1e5064f4fb113cc0cf90130fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "955d4ef517294f54a545c1fb9b0a1dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b82d4cfb4bba42229ff6928d5c3a83d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bbe6d56bb14607b579276b6df78b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a31c3db7b748eca3f1692e8ea80964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_465a793b471f42bdb902128ae336ab19",
              "IPY_MODEL_7c8538c8041e451d9f532f9843b20387",
              "IPY_MODEL_fe99f3c2ff9f4bdf892700fe6039624a"
            ],
            "layout": "IPY_MODEL_61fb9632bb124d6ba3abaef71a49ec89"
          }
        },
        "465a793b471f42bdb902128ae336ab19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cdbf52edaa74fafa480b44aff57b0aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_faf7d87546984096862a91b0111575b0",
            "value": "Tokenizingâ€‡eval:â€‡100%"
          }
        },
        "7c8538c8041e451d9f532f9843b20387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ae62fc707d4f3da81a95a32f8522ec",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b3fa324cf044e7e9bb9b32c7864a6dc",
            "value": 99
          }
        },
        "fe99f3c2ff9f4bdf892700fe6039624a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_661b175e46f24ba39f75c23ff74e2d83",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c17e72e49a442e7adf8247e0a2a0d52",
            "value": "â€‡99/99â€‡[00:00&lt;00:00,â€‡498.53â€‡examples/s]"
          }
        },
        "61fb9632bb124d6ba3abaef71a49ec89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdbf52edaa74fafa480b44aff57b0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf7d87546984096862a91b0111575b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ae62fc707d4f3da81a95a32f8522ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3fa324cf044e7e9bb9b32c7864a6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "661b175e46f24ba39f75c23ff74e2d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c17e72e49a442e7adf8247e0a2a0d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "94ce1350095344f59dc29be634067c03",
            "adcdfeee248445f9af7f7ee516aa8c3d",
            "7f5016961ec446129e38a59aa28847c0",
            "7fe52742f6da464e895618c810677387",
            "acf3320043944a568059d6df442c975c",
            "c80ce414db0a4d7bab554b19c14b967a",
            "bc392b5aa16449c18749e3c33c643d7b",
            "aa9525b1e5064f4fb113cc0cf90130fa",
            "955d4ef517294f54a545c1fb9b0a1dac",
            "b82d4cfb4bba42229ff6928d5c3a83d5",
            "27bbe6d56bb14607b579276b6df78b31",
            "d4a31c3db7b748eca3f1692e8ea80964",
            "465a793b471f42bdb902128ae336ab19",
            "7c8538c8041e451d9f532f9843b20387",
            "fe99f3c2ff9f4bdf892700fe6039624a",
            "61fb9632bb124d6ba3abaef71a49ec89",
            "9cdbf52edaa74fafa480b44aff57b0aa",
            "faf7d87546984096862a91b0111575b0",
            "c7ae62fc707d4f3da81a95a32f8522ec",
            "2b3fa324cf044e7e9bb9b32c7864a6dc",
            "661b175e46f24ba39f75c23ff74e2d83",
            "6c17e72e49a442e7adf8247e0a2a0d52"
          ]
        },
        "id": "eVt1HHVpiwny",
        "outputId": "5474e762-3533-4910-f838-2fac09cc4a36"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENVIRONMENT\n",
            "============================================================\n",
            "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "PyTorch: 2.8.0+cu126\n",
            "GPU: Tesla T4 | CUDA: 12.6\n",
            "============================================================\n",
            "============================================================\n",
            "GPU CONFIGURATION\n",
            "============================================================\n",
            "GPU Available: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.74 GB\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-231cbbee-a502-4406-a999-6645a25daf5c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-231cbbee-a502-4406-a999-6645a25daf5c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving medquadCardiovascular.csv to medquadCardiovascular (2).csv\n",
            "Dataset CSV: medquadCardiovascular (2).csv\n",
            "\n",
            "============================================================\n",
            "LOADING CARDIOVASCULAR DATASET\n",
            "============================================================\n",
            "Total records: 654\n",
            "Columns: ['question', 'answer', 'source', 'focus_area']\n",
            "Sample question: What is (are) High Blood Pressure ?...\n",
            "Sample answer chars: 5586\n",
            "Train: 555 | Val: 99\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL AND TOKENIZER\n",
            "============================================================\n",
            "Model: dmis-lab/biobert-base-cased-v1.1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TOKENIZING DATASET\n",
            "============================================================\n",
            "Tokenizing train...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94ce1350095344f59dc29be634067c03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/555 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing eval...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4a31c3db7b748eca3f1692e8ea80964",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval:   0%|          | 0/99 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "============================================================\n",
            "Metrics ready.\n",
            "\n",
            "============================================================\n",
            "TRAINING CONFIGURATION (T4 GPU OPTIMIZED)\n",
            "============================================================\n",
            "T4 GPU Configuration:\n",
            "  - Batch Size: 8\n",
            "  - Gradient Accumulation: 4\n",
            "  - Effective Batch Size: 32\n",
            "  - FP16 Enabled: True\n",
            "  - Learning Rate: 2e-05\n",
            "  - Epochs: 5\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INITIALIZING TRAINER\n",
            "============================================================\n",
            "Trainer ready.\n",
            "\n",
            "============================================================\n",
            "RANDOM SEARCH HYPERPARAMETER TUNING (T4 GPU OPTIMIZED)\n",
            "============================================================\n",
            "ðŸ” Random Search Strategy\n",
            "Total search space size: 384 possible combinations\n",
            "Random sampling: 10 configurations (2.6% coverage)\n",
            "Fixed batch size: 8 (optimal for T4 16GB memory)\n",
            "Gradient accumulation: 4 (effective batch size: 32)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 1/10\n",
            "Params: Epochs=7, LR=3e-05, Warmup=0.1, Weight Decay=0.01\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.796499490737915, 'eval_exact_match': 0.07281553398058252, 'eval_start_accuracy': 0.9951456310679612, 'eval_end_accuracy': 0.07281553398058252, 'eval_f1': 0.9048256275897774, 'eval_runtime': 1.2843, 'eval_samples_per_second': 160.404, 'eval_steps_per_second': 20.245, 'epoch': 1.0}\n",
            "{'eval_loss': 1.299936294555664, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.9639346070225541, 'eval_runtime': 1.2874, 'eval_samples_per_second': 160.018, 'eval_steps_per_second': 20.196, 'epoch': 2.0}\n",
            "{'eval_loss': 1.374715805053711, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.9759603354716637, 'eval_runtime': 1.2295, 'eval_samples_per_second': 167.552, 'eval_steps_per_second': 21.147, 'epoch': 3.0}\n",
            "{'eval_loss': 1.4290721416473389, 'eval_exact_match': 0.18932038834951456, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.18932038834951456, 'eval_f1': 0.968422873209547, 'eval_runtime': 1.2522, 'eval_samples_per_second': 164.507, 'eval_steps_per_second': 20.763, 'epoch': 4.0}\n",
            "{'eval_loss': 1.3109320402145386, 'eval_exact_match': 0.19902912621359223, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.19902912621359223, 'eval_f1': 0.9753508775982958, 'eval_runtime': 1.2419, 'eval_samples_per_second': 165.874, 'eval_steps_per_second': 20.936, 'epoch': 5.0}\n",
            "{'eval_loss': 1.2277101278305054, 'eval_exact_match': 0.22330097087378642, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.22330097087378642, 'eval_f1': 0.9740292603782613, 'eval_runtime': 1.3201, 'eval_samples_per_second': 156.053, 'eval_steps_per_second': 19.696, 'epoch': 6.0}\n",
            "{'eval_loss': 1.2722008228302002, 'eval_exact_match': 0.22815533980582525, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.22815533980582525, 'eval_f1': 0.9755742703386535, 'eval_runtime': 1.239, 'eval_samples_per_second': 166.266, 'eval_steps_per_second': 20.985, 'epoch': 7.0}\n",
            "{'train_runtime': 160.1687, 'train_samples_per_second': 46.326, 'train_steps_per_second': 1.486, 'train_loss': 1.548535002379858, 'epoch': 7.0}\n",
            "{'eval_loss': 1.2722008228302002, 'eval_exact_match': 0.22815533980582525, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.22815533980582525, 'eval_f1': 0.9755742703386535, 'eval_runtime': 1.215, 'eval_samples_per_second': 169.546, 'eval_steps_per_second': 21.399, 'epoch': 7.0}\n",
            "âœ… Iteration 1 done â€” F1: 0.9756, Accuracy: 0.4854, Time: 160.72s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 2/10\n",
            "Params: Epochs=7, LR=4e-05, Warmup=0.1, Weight Decay=0.01\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.6042670011520386, 'eval_exact_match': 0.0970873786407767, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.0970873786407767, 'eval_f1': 0.9140144410515858, 'eval_runtime': 1.2634, 'eval_samples_per_second': 163.055, 'eval_steps_per_second': 20.58, 'epoch': 1.0}\n",
            "{'eval_loss': 1.3398340940475464, 'eval_exact_match': 0.1262135922330097, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1262135922330097, 'eval_f1': 0.955123004577997, 'eval_runtime': 1.2442, 'eval_samples_per_second': 165.562, 'eval_steps_per_second': 20.896, 'epoch': 2.0}\n",
            "{'eval_loss': 1.4142769575119019, 'eval_exact_match': 0.1796116504854369, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1796116504854369, 'eval_f1': 0.9629898507727912, 'eval_runtime': 1.2421, 'eval_samples_per_second': 165.85, 'eval_steps_per_second': 20.933, 'epoch': 3.0}\n",
            "{'eval_loss': 1.5667036771774292, 'eval_exact_match': 0.17475728155339806, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.17475728155339806, 'eval_f1': 0.9727344483598583, 'eval_runtime': 1.2405, 'eval_samples_per_second': 166.061, 'eval_steps_per_second': 20.959, 'epoch': 4.0}\n",
            "{'eval_loss': 1.5551213026046753, 'eval_exact_match': 0.21359223300970873, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.21359223300970873, 'eval_f1': 0.9767953071992982, 'eval_runtime': 1.3122, 'eval_samples_per_second': 156.99, 'eval_steps_per_second': 19.814, 'epoch': 5.0}\n",
            "{'eval_loss': 1.5674896240234375, 'eval_exact_match': 0.19902912621359223, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.19902912621359223, 'eval_f1': 0.9772415520539387, 'eval_runtime': 1.2317, 'eval_samples_per_second': 167.249, 'eval_steps_per_second': 21.109, 'epoch': 6.0}\n",
            "{'eval_loss': 1.545394778251648, 'eval_exact_match': 0.20388349514563106, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.20388349514563106, 'eval_f1': 0.9694486419932212, 'eval_runtime': 1.2401, 'eval_samples_per_second': 166.116, 'eval_steps_per_second': 20.966, 'epoch': 7.0}\n",
            "{'train_runtime': 159.3167, 'train_samples_per_second': 46.574, 'train_steps_per_second': 1.494, 'train_loss': 1.3925924862132353, 'epoch': 7.0}\n",
            "{'eval_loss': 1.545394778251648, 'eval_exact_match': 0.20388349514563106, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.20388349514563106, 'eval_f1': 0.9694486419932212, 'eval_runtime': 1.22, 'eval_samples_per_second': 168.848, 'eval_steps_per_second': 21.311, 'epoch': 7.0}\n",
            "âœ… Iteration 2 done â€” F1: 0.9694, Accuracy: 0.4693, Time: 159.83s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 3/10\n",
            "Params: Epochs=5, LR=2e-05, Warmup=0.2, Weight Decay=0.01\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 3.7305991649627686, 'eval_exact_match': 0.038834951456310676, 'eval_start_accuracy': 0.8932038834951457, 'eval_end_accuracy': 0.043689320388349516, 'eval_f1': 0.8325320713821402, 'eval_runtime': 1.2253, 'eval_samples_per_second': 168.127, 'eval_steps_per_second': 21.22, 'epoch': 1.0}\n",
            "{'eval_loss': 1.4357714653015137, 'eval_exact_match': 0.10679611650485436, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.10679611650485436, 'eval_f1': 0.9450301416934456, 'eval_runtime': 1.2484, 'eval_samples_per_second': 165.011, 'eval_steps_per_second': 20.827, 'epoch': 2.0}\n",
            "{'eval_loss': 1.3951047658920288, 'eval_exact_match': 0.14563106796116504, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.14563106796116504, 'eval_f1': 0.9606312069877889, 'eval_runtime': 1.2475, 'eval_samples_per_second': 165.128, 'eval_steps_per_second': 20.841, 'epoch': 3.0}\n",
            "{'eval_loss': 1.3321443796157837, 'eval_exact_match': 0.09223300970873786, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.09223300970873786, 'eval_f1': 0.9527327294556466, 'eval_runtime': 1.324, 'eval_samples_per_second': 155.593, 'eval_steps_per_second': 19.638, 'epoch': 4.0}\n",
            "{'eval_loss': 1.3429956436157227, 'eval_exact_match': 0.12135922330097088, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.12135922330097088, 'eval_f1': 0.9627542984090258, 'eval_runtime': 1.2384, 'eval_samples_per_second': 166.344, 'eval_steps_per_second': 20.995, 'epoch': 5.0}\n",
            "{'train_runtime': 114.1533, 'train_samples_per_second': 46.429, 'train_steps_per_second': 1.489, 'train_loss': 2.2598993637982536, 'epoch': 5.0}\n",
            "{'eval_loss': 1.3429956436157227, 'eval_exact_match': 0.12135922330097088, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.12135922330097088, 'eval_f1': 0.9627542984090258, 'eval_runtime': 1.2642, 'eval_samples_per_second': 162.946, 'eval_steps_per_second': 20.566, 'epoch': 5.0}\n",
            "âœ… Iteration 3 done â€” F1: 0.9628, Accuracy: 0.4142, Time: 114.62s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 4/10\n",
            "Params: Epochs=7, LR=4e-05, Warmup=0.25, Weight Decay=0.05\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 3.374480962753296, 'eval_exact_match': 0.03398058252427184, 'eval_start_accuracy': 0.8786407766990292, 'eval_end_accuracy': 0.038834951456310676, 'eval_f1': 0.8132797977374571, 'eval_runtime': 1.2901, 'eval_samples_per_second': 159.681, 'eval_steps_per_second': 20.154, 'epoch': 1.0}\n",
            "{'eval_loss': 1.377065658569336, 'eval_exact_match': 0.1796116504854369, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1796116504854369, 'eval_f1': 0.9403117005968528, 'eval_runtime': 1.2319, 'eval_samples_per_second': 167.217, 'eval_steps_per_second': 21.105, 'epoch': 2.0}\n",
            "{'eval_loss': 1.2493098974227905, 'eval_exact_match': 0.2087378640776699, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.2087378640776699, 'eval_f1': 0.9719198609483811, 'eval_runtime': 1.2422, 'eval_samples_per_second': 165.834, 'eval_steps_per_second': 20.931, 'epoch': 3.0}\n",
            "{'eval_loss': 1.3875086307525635, 'eval_exact_match': 0.1941747572815534, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1941747572815534, 'eval_f1': 0.9769419736715726, 'eval_runtime': 1.2409, 'eval_samples_per_second': 166.01, 'eval_steps_per_second': 20.953, 'epoch': 4.0}\n",
            "{'eval_loss': 1.072557806968689, 'eval_exact_match': 0.30097087378640774, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.30097087378640774, 'eval_f1': 0.9869380788892546, 'eval_runtime': 1.3086, 'eval_samples_per_second': 157.422, 'eval_steps_per_second': 19.869, 'epoch': 5.0}\n",
            "{'eval_loss': 0.9833978414535522, 'eval_exact_match': 0.33980582524271846, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.33980582524271846, 'eval_f1': 0.9889262147284616, 'eval_runtime': 1.2317, 'eval_samples_per_second': 167.248, 'eval_steps_per_second': 21.109, 'epoch': 6.0}\n",
            "{'eval_loss': 0.973896861076355, 'eval_exact_match': 0.3737864077669903, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.3737864077669903, 'eval_f1': 0.9896115526554323, 'eval_runtime': 1.2358, 'eval_samples_per_second': 166.697, 'eval_steps_per_second': 21.039, 'epoch': 7.0}\n",
            "{'train_runtime': 159.4861, 'train_samples_per_second': 46.524, 'train_steps_per_second': 1.492, 'train_loss': 1.6693788416245405, 'epoch': 7.0}\n",
            "{'eval_loss': 0.973896861076355, 'eval_exact_match': 0.3737864077669903, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.3737864077669903, 'eval_f1': 0.9896115526554323, 'eval_runtime': 1.2008, 'eval_samples_per_second': 171.547, 'eval_steps_per_second': 21.652, 'epoch': 7.0}\n",
            "âœ… Iteration 4 done â€” F1: 0.9896, Accuracy: 0.5825, Time: 159.92s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 5/10\n",
            "Params: Epochs=10, LR=2e-05, Warmup=0.15, Weight Decay=0.0\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 4.492733001708984, 'eval_exact_match': 0.02912621359223301, 'eval_start_accuracy': 0.8203883495145631, 'eval_end_accuracy': 0.02912621359223301, 'eval_f1': 0.7310091912241773, 'eval_runtime': 1.2477, 'eval_samples_per_second': 165.105, 'eval_steps_per_second': 20.839, 'epoch': 1.0}\n",
            "{'eval_loss': 1.4522026777267456, 'eval_exact_match': 0.10679611650485436, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.10679611650485436, 'eval_f1': 0.9414299218693352, 'eval_runtime': 1.2419, 'eval_samples_per_second': 165.878, 'eval_steps_per_second': 20.936, 'epoch': 2.0}\n",
            "{'eval_loss': 1.4439759254455566, 'eval_exact_match': 0.15048543689320387, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.15048543689320387, 'eval_f1': 0.9626081203561228, 'eval_runtime': 1.237, 'eval_samples_per_second': 166.538, 'eval_steps_per_second': 21.019, 'epoch': 3.0}\n",
            "{'eval_loss': 1.422602653503418, 'eval_exact_match': 0.1553398058252427, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1553398058252427, 'eval_f1': 0.9620055773392594, 'eval_runtime': 1.2985, 'eval_samples_per_second': 158.648, 'eval_steps_per_second': 20.024, 'epoch': 4.0}\n",
            "{'eval_loss': 1.3534259796142578, 'eval_exact_match': 0.17475728155339806, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.17475728155339806, 'eval_f1': 0.9552720053236912, 'eval_runtime': 1.2405, 'eval_samples_per_second': 166.063, 'eval_steps_per_second': 20.959, 'epoch': 5.0}\n",
            "{'eval_loss': 1.387430191040039, 'eval_exact_match': 0.16990291262135923, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16990291262135923, 'eval_f1': 0.9679002218140627, 'eval_runtime': 1.235, 'eval_samples_per_second': 166.802, 'eval_steps_per_second': 21.053, 'epoch': 6.0}\n",
            "{'eval_loss': 1.348002314567566, 'eval_exact_match': 0.2524271844660194, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.2524271844660194, 'eval_f1': 0.9715021482917968, 'eval_runtime': 1.2411, 'eval_samples_per_second': 165.986, 'eval_steps_per_second': 20.95, 'epoch': 7.0}\n",
            "{'eval_loss': 1.3045313358306885, 'eval_exact_match': 0.27184466019417475, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.27184466019417475, 'eval_f1': 0.9769368394395593, 'eval_runtime': 1.2273, 'eval_samples_per_second': 167.844, 'eval_steps_per_second': 21.184, 'epoch': 8.0}\n",
            "{'eval_loss': 1.201498031616211, 'eval_exact_match': 0.2766990291262136, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.2766990291262136, 'eval_f1': 0.9820511023862638, 'eval_runtime': 1.3233, 'eval_samples_per_second': 155.67, 'eval_steps_per_second': 19.648, 'epoch': 9.0}\n",
            "{'eval_loss': 1.2080745697021484, 'eval_exact_match': 0.27184466019417475, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.27184466019417475, 'eval_f1': 0.9821592236898882, 'eval_runtime': 1.2342, 'eval_samples_per_second': 166.905, 'eval_steps_per_second': 21.066, 'epoch': 10.0}\n",
            "{'train_runtime': 226.6667, 'train_samples_per_second': 46.765, 'train_steps_per_second': 1.5, 'train_loss': 1.5881841322954964, 'epoch': 10.0}\n",
            "{'eval_loss': 1.2080745697021484, 'eval_exact_match': 0.27184466019417475, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.27184466019417475, 'eval_f1': 0.9821592236898882, 'eval_runtime': 1.2074, 'eval_samples_per_second': 170.613, 'eval_steps_per_second': 21.534, 'epoch': 10.0}\n",
            "âœ… Iteration 5 done â€” F1: 0.9822, Accuracy: 0.5146, Time: 227.19s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 6/10\n",
            "Params: Epochs=5, LR=3e-05, Warmup=0.15, Weight Decay=0.001\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.9769604206085205, 'eval_exact_match': 0.11650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.11650485436893204, 'eval_f1': 0.9036416924978704, 'eval_runtime': 1.2419, 'eval_samples_per_second': 165.878, 'eval_steps_per_second': 20.936, 'epoch': 1.0}\n",
            "{'eval_loss': 1.3570940494537354, 'eval_exact_match': 0.13592233009708737, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.13592233009708737, 'eval_f1': 0.941005388032972, 'eval_runtime': 1.2451, 'eval_samples_per_second': 165.446, 'eval_steps_per_second': 20.882, 'epoch': 2.0}\n",
            "{'eval_loss': 1.2580759525299072, 'eval_exact_match': 0.13592233009708737, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.13592233009708737, 'eval_f1': 0.9581584508904503, 'eval_runtime': 1.241, 'eval_samples_per_second': 166.001, 'eval_steps_per_second': 20.952, 'epoch': 3.0}\n",
            "{'eval_loss': 1.3556081056594849, 'eval_exact_match': 0.1650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1650485436893204, 'eval_f1': 0.9626774265373546, 'eval_runtime': 1.252, 'eval_samples_per_second': 164.534, 'eval_steps_per_second': 20.766, 'epoch': 4.0}\n",
            "{'eval_loss': 1.3399039506912231, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.9639653705702118, 'eval_runtime': 1.3095, 'eval_samples_per_second': 157.309, 'eval_steps_per_second': 19.855, 'epoch': 5.0}\n",
            "{'train_runtime': 113.8533, 'train_samples_per_second': 46.551, 'train_steps_per_second': 1.493, 'train_loss': 1.93123779296875, 'epoch': 5.0}\n",
            "{'eval_loss': 1.3399039506912231, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.9639653705702118, 'eval_runtime': 1.2906, 'eval_samples_per_second': 159.615, 'eval_steps_per_second': 20.146, 'epoch': 5.0}\n",
            "âœ… Iteration 6 done â€” F1: 0.9640, Accuracy: 0.4401, Time: 114.38s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 7/10\n",
            "Params: Epochs=5, LR=3e-05, Warmup=0.1, Weight Decay=0.0\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.7630094289779663, 'eval_exact_match': 0.07766990291262135, 'eval_start_accuracy': 0.9902912621359223, 'eval_end_accuracy': 0.07766990291262135, 'eval_f1': 0.9025949671280988, 'eval_runtime': 1.3115, 'eval_samples_per_second': 157.066, 'eval_steps_per_second': 19.824, 'epoch': 1.0}\n",
            "{'eval_loss': 1.3046009540557861, 'eval_exact_match': 0.13592233009708737, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.13592233009708737, 'eval_f1': 0.9379490005725937, 'eval_runtime': 1.2354, 'eval_samples_per_second': 166.745, 'eval_steps_per_second': 21.046, 'epoch': 2.0}\n",
            "{'eval_loss': 1.2336182594299316, 'eval_exact_match': 0.17475728155339806, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.17475728155339806, 'eval_f1': 0.9360799224419735, 'eval_runtime': 1.2361, 'eval_samples_per_second': 166.647, 'eval_steps_per_second': 21.033, 'epoch': 3.0}\n",
            "{'eval_loss': 1.314382553100586, 'eval_exact_match': 0.18446601941747573, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.18446601941747573, 'eval_f1': 0.9458633327950601, 'eval_runtime': 1.2362, 'eval_samples_per_second': 166.642, 'eval_steps_per_second': 21.032, 'epoch': 4.0}\n",
            "{'eval_loss': 1.2888116836547852, 'eval_exact_match': 0.19902912621359223, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.19902912621359223, 'eval_f1': 0.9470460569774825, 'eval_runtime': 1.2474, 'eval_samples_per_second': 165.145, 'eval_steps_per_second': 20.844, 'epoch': 5.0}\n",
            "{'train_runtime': 113.3793, 'train_samples_per_second': 46.746, 'train_steps_per_second': 1.499, 'train_loss': 1.8536069982192096, 'epoch': 5.0}\n",
            "{'eval_loss': 1.2888116836547852, 'eval_exact_match': 0.19902912621359223, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.19902912621359223, 'eval_f1': 0.9470460569774825, 'eval_runtime': 1.2153, 'eval_samples_per_second': 169.5, 'eval_steps_per_second': 21.393, 'epoch': 5.0}\n",
            "âœ… Iteration 7 done â€” F1: 0.9470, Accuracy: 0.4660, Time: 113.81s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 8/10\n",
            "Params: Epochs=10, LR=1.5e-05, Warmup=0.15, Weight Decay=0.0\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.2143025398254395, 'eval_exact_match': 0.0048543689320388345, 'eval_start_accuracy': 0.49029126213592233, 'eval_end_accuracy': 0.0048543689320388345, 'eval_f1': 0.5293958203648358, 'eval_runtime': 1.2457, 'eval_samples_per_second': 165.373, 'eval_steps_per_second': 20.872, 'epoch': 1.0}\n",
            "{'eval_loss': 1.6556382179260254, 'eval_exact_match': 0.06310679611650485, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.06310679611650485, 'eval_f1': 0.9113654804787183, 'eval_runtime': 1.3099, 'eval_samples_per_second': 157.263, 'eval_steps_per_second': 19.849, 'epoch': 2.0}\n",
            "{'eval_loss': 1.2863277196884155, 'eval_exact_match': 0.14563106796116504, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.14563106796116504, 'eval_f1': 0.9378640417563603, 'eval_runtime': 1.2301, 'eval_samples_per_second': 167.472, 'eval_steps_per_second': 21.137, 'epoch': 3.0}\n",
            "{'eval_loss': 1.2401901483535767, 'eval_exact_match': 0.1407766990291262, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1407766990291262, 'eval_f1': 0.9373306318778877, 'eval_runtime': 1.2438, 'eval_samples_per_second': 165.617, 'eval_steps_per_second': 20.903, 'epoch': 4.0}\n",
            "{'eval_loss': 1.1289230585098267, 'eval_exact_match': 0.11650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.11650485436893204, 'eval_f1': 0.9386497588228624, 'eval_runtime': 1.2358, 'eval_samples_per_second': 166.69, 'eval_steps_per_second': 21.039, 'epoch': 5.0}\n",
            "{'eval_loss': 0.9575945138931274, 'eval_exact_match': 0.2961165048543689, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.2961165048543689, 'eval_f1': 0.9533820859839717, 'eval_runtime': 1.2507, 'eval_samples_per_second': 164.713, 'eval_steps_per_second': 20.789, 'epoch': 6.0}\n",
            "{'eval_loss': 0.9202553629875183, 'eval_exact_match': 0.34951456310679613, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.34951456310679613, 'eval_f1': 0.9559901512292732, 'eval_runtime': 1.2801, 'eval_samples_per_second': 160.921, 'eval_steps_per_second': 20.31, 'epoch': 7.0}\n",
            "{'eval_loss': 0.7991766929626465, 'eval_exact_match': 0.441747572815534, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.441747572815534, 'eval_f1': 0.9570201454256727, 'eval_runtime': 1.2332, 'eval_samples_per_second': 167.045, 'eval_steps_per_second': 21.083, 'epoch': 8.0}\n",
            "{'eval_loss': 0.7620470523834229, 'eval_exact_match': 0.4174757281553398, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.4174757281553398, 'eval_f1': 0.9568076952427548, 'eval_runtime': 1.2362, 'eval_samples_per_second': 166.64, 'eval_steps_per_second': 21.032, 'epoch': 9.0}\n",
            "{'eval_loss': 0.7314262986183167, 'eval_exact_match': 0.46601941747572817, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.46601941747572817, 'eval_f1': 0.9528076938367921, 'eval_runtime': 1.23, 'eval_samples_per_second': 167.484, 'eval_steps_per_second': 21.139, 'epoch': 10.0}\n",
            "{'train_runtime': 226.7378, 'train_samples_per_second': 46.75, 'train_steps_per_second': 1.5, 'train_loss': 1.709273753446691, 'epoch': 10.0}\n",
            "{'eval_loss': 0.7314262986183167, 'eval_exact_match': 0.46601941747572817, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.46601941747572817, 'eval_f1': 0.9528076938367921, 'eval_runtime': 1.2116, 'eval_samples_per_second': 170.024, 'eval_steps_per_second': 21.459, 'epoch': 10.0}\n",
            "âœ… Iteration 8 done â€” F1: 0.9528, Accuracy: 0.6440, Time: 227.16s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 9/10\n",
            "Params: Epochs=10, LR=1e-05, Warmup=0.1, Weight Decay=0.01\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.005612373352051, 'eval_exact_match': 0.0048543689320388345, 'eval_start_accuracy': 0.27184466019417475, 'eval_end_accuracy': 0.024271844660194174, 'eval_f1': 0.6746709486934298, 'eval_runtime': 1.2263, 'eval_samples_per_second': 167.982, 'eval_steps_per_second': 21.202, 'epoch': 1.0}\n",
            "{'eval_loss': 1.936866044998169, 'eval_exact_match': 0.06310679611650485, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.06310679611650485, 'eval_f1': 0.9096074151226867, 'eval_runtime': 1.2772, 'eval_samples_per_second': 161.296, 'eval_steps_per_second': 20.358, 'epoch': 2.0}\n",
            "{'eval_loss': 1.3902051448822021, 'eval_exact_match': 0.11650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.11650485436893204, 'eval_f1': 0.9510415797681987, 'eval_runtime': 1.3003, 'eval_samples_per_second': 158.429, 'eval_steps_per_second': 19.996, 'epoch': 3.0}\n",
            "{'eval_loss': 1.355849027633667, 'eval_exact_match': 0.1407766990291262, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1407766990291262, 'eval_f1': 0.9502704669985605, 'eval_runtime': 1.2325, 'eval_samples_per_second': 167.143, 'eval_steps_per_second': 21.096, 'epoch': 4.0}\n",
            "{'eval_loss': 1.2598146200180054, 'eval_exact_match': 0.1650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1650485436893204, 'eval_f1': 0.9492142009192719, 'eval_runtime': 1.2191, 'eval_samples_per_second': 168.983, 'eval_steps_per_second': 21.328, 'epoch': 5.0}\n",
            "{'eval_loss': 1.2506529092788696, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.954859907821771, 'eval_runtime': 1.2411, 'eval_samples_per_second': 165.982, 'eval_steps_per_second': 20.949, 'epoch': 6.0}\n",
            "{'eval_loss': 1.2103345394134521, 'eval_exact_match': 0.1553398058252427, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1553398058252427, 'eval_f1': 0.9556614973669316, 'eval_runtime': 1.3281, 'eval_samples_per_second': 155.112, 'eval_steps_per_second': 19.577, 'epoch': 7.0}\n",
            "{'eval_loss': 1.211704969406128, 'eval_exact_match': 0.16990291262135923, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16990291262135923, 'eval_f1': 0.9565169281379376, 'eval_runtime': 1.2385, 'eval_samples_per_second': 166.329, 'eval_steps_per_second': 20.993, 'epoch': 8.0}\n",
            "{'eval_loss': 1.2096226215362549, 'eval_exact_match': 0.18446601941747573, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.18446601941747573, 'eval_f1': 0.952399597483968, 'eval_runtime': 1.2485, 'eval_samples_per_second': 165.0, 'eval_steps_per_second': 20.825, 'epoch': 9.0}\n",
            "{'eval_loss': 1.210649013519287, 'eval_exact_match': 0.1796116504854369, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1796116504854369, 'eval_f1': 0.9618135451789697, 'eval_runtime': 1.242, 'eval_samples_per_second': 165.865, 'eval_steps_per_second': 20.934, 'epoch': 10.0}\n",
            "{'train_runtime': 228.0671, 'train_samples_per_second': 46.478, 'train_steps_per_second': 1.491, 'train_loss': 1.8974318560431986, 'epoch': 10.0}\n",
            "{'eval_loss': 1.210649013519287, 'eval_exact_match': 0.1796116504854369, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1796116504854369, 'eval_f1': 0.9618135451789697, 'eval_runtime': 1.2114, 'eval_samples_per_second': 170.055, 'eval_steps_per_second': 21.463, 'epoch': 10.0}\n",
            "âœ… Iteration 9 done â€” F1: 0.9618, Accuracy: 0.4531, Time: 228.5s\n",
            "\n",
            "============================================================\n",
            "â–¶ï¸ Random Search Config 10/10\n",
            "Params: Epochs=7, LR=1.5e-05, Warmup=0.25, Weight Decay=0.05\n",
            "============================================================\n",
            "ðŸ”„ Reloading fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.138084411621094, 'eval_exact_match': 0.0048543689320388345, 'eval_start_accuracy': 0.22330097087378642, 'eval_end_accuracy': 0.024271844660194174, 'eval_f1': 0.6579431479494263, 'eval_runtime': 1.2461, 'eval_samples_per_second': 165.316, 'eval_steps_per_second': 20.865, 'epoch': 1.0}\n",
            "{'eval_loss': 1.7718418836593628, 'eval_exact_match': 0.07766990291262135, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.07766990291262135, 'eval_f1': 0.9250702990122049, 'eval_runtime': 1.2266, 'eval_samples_per_second': 167.944, 'eval_steps_per_second': 21.197, 'epoch': 2.0}\n",
            "{'eval_loss': 1.3364888429641724, 'eval_exact_match': 0.1650485436893204, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1650485436893204, 'eval_f1': 0.941182133397326, 'eval_runtime': 1.2841, 'eval_samples_per_second': 160.424, 'eval_steps_per_second': 20.248, 'epoch': 3.0}\n",
            "{'eval_loss': 1.3074567317962646, 'eval_exact_match': 0.14563106796116504, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.14563106796116504, 'eval_f1': 0.9479277652919159, 'eval_runtime': 1.2828, 'eval_samples_per_second': 160.582, 'eval_steps_per_second': 20.268, 'epoch': 4.0}\n",
            "{'eval_loss': 1.20835280418396, 'eval_exact_match': 0.16019417475728157, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.16019417475728157, 'eval_f1': 0.9597274640314596, 'eval_runtime': 1.2476, 'eval_samples_per_second': 165.121, 'eval_steps_per_second': 20.841, 'epoch': 5.0}\n",
            "{'eval_loss': 1.198588252067566, 'eval_exact_match': 0.17475728155339806, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.17475728155339806, 'eval_f1': 0.9585065187806564, 'eval_runtime': 1.2431, 'eval_samples_per_second': 165.718, 'eval_steps_per_second': 20.916, 'epoch': 6.0}\n",
            "{'eval_loss': 1.1413836479187012, 'eval_exact_match': 0.1941747572815534, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1941747572815534, 'eval_f1': 0.9581574120954588, 'eval_runtime': 1.2113, 'eval_samples_per_second': 170.064, 'eval_steps_per_second': 21.464, 'epoch': 7.0}\n",
            "{'train_runtime': 159.8168, 'train_samples_per_second': 46.428, 'train_steps_per_second': 1.489, 'train_loss': 2.186404444590336, 'epoch': 7.0}\n",
            "{'eval_loss': 1.1413836479187012, 'eval_exact_match': 0.1941747572815534, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.1941747572815534, 'eval_f1': 0.9581574120954588, 'eval_runtime': 1.2068, 'eval_samples_per_second': 170.697, 'eval_steps_per_second': 21.544, 'epoch': 7.0}\n",
            "âœ… Iteration 10 done â€” F1: 0.9582, Accuracy: 0.4628, Time: 160.25s\n",
            "\n",
            "âœ… All 10 random search runs completed successfully!\n",
            "ðŸ“Š Final results saved in one Excel file:\n",
            "âž¡ï¸ /content/BioBERT_Random_Search_Results.xlsx\n",
            "\n",
            "ðŸ” Random Search Summary:\n",
            "   - Total search space: 384 combinations\n",
            "   - Explored: 10 configurations (2.6% coverage)\n",
            "   - Advantage: More efficient exploration than Grid Search\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "ðŸš€ Training in progress...\n",
            "\n",
            "{'eval_loss': 1.1004924774169922, 'eval_exact_match': 0.2087378640776699, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.2087378640776699, 'eval_f1': 0.9551353419699823, 'eval_runtime': 1.2319, 'eval_samples_per_second': 167.22, 'eval_steps_per_second': 21.105, 'epoch': 1.0}\n",
            "{'eval_loss': 0.9653071165084839, 'eval_exact_match': 0.30097087378640774, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.30097087378640774, 'eval_f1': 0.967598487913296, 'eval_runtime': 1.3077, 'eval_samples_per_second': 157.524, 'eval_steps_per_second': 19.882, 'epoch': 2.0}\n",
            "{'eval_loss': 0.9567452669143677, 'eval_exact_match': 0.3446601941747573, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.3446601941747573, 'eval_f1': 0.9660725562848815, 'eval_runtime': 1.2392, 'eval_samples_per_second': 166.232, 'eval_steps_per_second': 20.981, 'epoch': 3.0}\n",
            "{'eval_loss': 0.9539965987205505, 'eval_exact_match': 0.3883495145631068, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.3883495145631068, 'eval_f1': 0.9712206917683774, 'eval_runtime': 1.2546, 'eval_samples_per_second': 164.197, 'eval_steps_per_second': 20.724, 'epoch': 4.0}\n",
            "{'eval_loss': 0.9694664478302002, 'eval_exact_match': 0.39805825242718446, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.39805825242718446, 'eval_f1': 0.9838770418016157, 'eval_runtime': 1.243, 'eval_samples_per_second': 165.726, 'eval_steps_per_second': 20.917, 'epoch': 5.0}\n",
            "{'eval_loss': 0.9579909443855286, 'eval_exact_match': 0.36893203883495146, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.36893203883495146, 'eval_f1': 0.9798176770591095, 'eval_runtime': 1.2753, 'eval_samples_per_second': 161.536, 'eval_steps_per_second': 20.388, 'epoch': 6.0}\n",
            "{'eval_loss': 0.8741251826286316, 'eval_exact_match': 0.45145631067961167, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.45145631067961167, 'eval_f1': 0.9808520636615159, 'eval_runtime': 1.2906, 'eval_samples_per_second': 159.614, 'eval_steps_per_second': 20.145, 'epoch': 7.0}\n",
            "{'train_runtime': 158.5558, 'train_samples_per_second': 46.797, 'train_steps_per_second': 1.501, 'train_loss': 0.8159002736836922, 'epoch': 7.0}\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETED\n",
            "============================================================\n",
            "Training Loss: 0.8159002736836922\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION\n",
            "============================================================\n",
            "{'eval_loss': 0.8741251826286316, 'eval_exact_match': 0.45145631067961167, 'eval_start_accuracy': 1.0, 'eval_end_accuracy': 0.45145631067961167, 'eval_f1': 0.9808520636615159, 'eval_runtime': 1.3017, 'eval_samples_per_second': 158.25, 'eval_steps_per_second': 19.973, 'epoch': 7.0}\n",
            "epoch: 7.0\n",
            "eval_end_accuracy: 0.45145631067961167\n",
            "eval_exact_match: 0.45145631067961167\n",
            "eval_f1: 0.9808520636615159\n",
            "eval_loss: 0.8741251826286316\n",
            "eval_runtime: 1.3017\n",
            "eval_samples_per_second: 158.25\n",
            "eval_start_accuracy: 1.0\n",
            "eval_steps_per_second: 19.973\n"
          ]
        }
      ],
      "source": [
        "# 1) Environment setup (Colab)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(packages):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n",
        "\n",
        "# Core ML stack\n",
        "pip_install([\n",
        "    \"transformers>=4.44.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.26.0\",\n",
        "    \"evaluate>=0.4.0\",\n",
        "])\n",
        "\n",
        "# Colab-specific checks\n",
        "try:\n",
        "    import torch\n",
        "    import platform\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ENVIRONMENT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Python: {sys.version.split()[0]} | Platform: {platform.platform()}\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"GPU not detected. Enable a GPU in Runtime > Change runtime type > T4/other.\")\n",
        "    print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(\"Environment check failed:\", e)\n",
        "\n",
        "# 2) Imports and GPU config\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import default_data_collator\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2)} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available; training will be slower.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 3) Dataset loading (Cardiovascular Q&A)\n",
        "# Option A: Mount Drive\n",
        "USE_DRIVE = False  # set True to use Drive\n",
        "CSV_PATH = \"\"       # e.g., \"/content/drive/MyDrive/medquadCardiovascular.csv\"\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Option B: Upload a file\n",
        "USE_UPLOAD = not USE_DRIVE\n",
        "if USE_UPLOAD:\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        uploaded = files.upload()\n",
        "        # Pick the first uploaded file\n",
        "        if uploaded:\n",
        "            CSV_PATH = list(uploaded.keys())[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not CSV_PATH:\n",
        "    # Fallback sample: you can place the CSV at a public URL and download it\n",
        "    # For now, raise an error to prompt the user.\n",
        "    raise ValueError(\"Please provide CSV_PATH via Drive or upload.\")\n",
        "\n",
        "print(\"Dataset CSV:\", CSV_PATH)\n",
        "\n",
        "# 4) Load and split dataset\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING CARDIOVASCULAR DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset = pd.read_csv(CSV_PATH)\n",
        "print(f\"Total records: {len(dataset)}\")\n",
        "print(f\"Columns: {list(dataset.columns)}\")\n",
        "print(f\"Sample question: {str(dataset.iloc[0]['question'])[:80]}...\")\n",
        "print(f\"Sample answer chars: {len(str(dataset.iloc[0]['answer']))}\")\n",
        "\n",
        "# Drop nulls\n",
        "dataset = dataset.dropna(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n",
        "\n",
        "# Train/val split (85/15)\n",
        "dataset_shuffled = dataset.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "split_idx = int(len(dataset_shuffled) * 0.85)\n",
        "train_data = dataset_shuffled.iloc[:split_idx].copy()\n",
        "eval_data = dataset_shuffled.iloc[split_idx:].copy()\n",
        "\n",
        "print(f\"Train: {len(train_data)} | Val: {len(eval_data)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 5) Model and tokenizer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING MODEL AND TOKENIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# 6) Tokenization and label prep (extractive QA)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOKENIZING DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_data)\n",
        "eval_ds = Dataset.from_pandas(eval_data)\n",
        "\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"answer\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "\n",
        "        context_start = None\n",
        "        context_end = None\n",
        "        for idx, seq_id in enumerate(sequence_ids):\n",
        "            if seq_id == 1:\n",
        "                if context_start is None:\n",
        "                    context_start = idx\n",
        "                context_end = idx\n",
        "\n",
        "        if context_start is None:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            answer_start = context_start\n",
        "            answer_end = min(context_start + 50, context_end)\n",
        "            start_positions.append(answer_start)\n",
        "            end_positions.append(answer_end)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "\n",
        "    # Drop offset_mapping so it isn't fed to the model\n",
        "    if \"offset_mapping\" in tokenized:\n",
        "        tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing train...\")\n",
        "tokenized_train = train_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        "    desc=\"Tokenizing train\",\n",
        ")\n",
        "\n",
        "print(\"Tokenizing eval...\")\n",
        "tokenized_eval = eval_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=eval_ds.column_names,\n",
        "    desc=\"Tokenizing eval\",\n",
        ")\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 7) Metrics\n",
        "import numpy as np\n",
        "\n",
        "def compute_qa_metrics(eval_pred):\n",
        "    predictions, label_ids = eval_pred\n",
        "    start_logits, end_logits = predictions\n",
        "\n",
        "    pred_starts = np.argmax(start_logits, axis=1)\n",
        "    pred_ends = np.argmax(end_logits, axis=1)\n",
        "\n",
        "    # Ensure label_ids is treated as a tuple\n",
        "    true_starts = np.asarray(label_ids[0]).reshape(-1)\n",
        "    true_ends = np.asarray(label_ids[1]).reshape(-1)\n",
        "\n",
        "    exact_match = np.mean((pred_starts == true_starts) & (pred_ends == true_ends))\n",
        "    start_accuracy = np.mean(pred_starts == true_starts)\n",
        "    end_accuracy = np.mean(pred_ends == true_ends)\n",
        "\n",
        "    f1_scores = []\n",
        "    for ps, pe, ts, te in zip(pred_starts, pred_ends, true_starts, true_ends):\n",
        "        ps, pe, ts, te = int(ps), int(pe), int(ts), int(te)\n",
        "        pred_tokens = set(range(ps, pe + 1))\n",
        "        true_tokens = set(range(ts, te + 1))\n",
        "        if not pred_tokens and not true_tokens:\n",
        "            f1_scores.append(1.0)\n",
        "        elif not pred_tokens or not true_tokens:\n",
        "            f1_scores.append(0.0)\n",
        "        else:\n",
        "            common = len(pred_tokens & true_tokens)\n",
        "            if common == 0:\n",
        "                f1_scores.append(0.0)\n",
        "            else:\n",
        "                precision = common / len(pred_tokens)\n",
        "                recall = common / len(true_tokens)\n",
        "                f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": float(exact_match),\n",
        "        \"start_accuracy\": float(start_accuracy),\n",
        "        \"end_accuracy\": float(end_accuracy),\n",
        "        \"f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
        "    }\n",
        "\n",
        "print(\"Metrics ready.\")\n",
        "\n",
        "# 8) Training configuration (T4 GPU Optimized)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING CONFIGURATION (T4 GPU OPTIMIZED)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_cardio_qa\",\n",
        "    num_train_epochs=5,  # Increased for better convergence\n",
        "    per_device_train_batch_size=8,  # Reduced for T4 memory (16GB)\n",
        "    per_device_eval_batch_size=8,  # Matched with train batch\n",
        "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size of 32\n",
        "    learning_rate=2e-5,  # Slightly lower for medical domain stability\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.15,  # More warmup for better stability\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=torch.cuda.is_available(),  # Essential for T4 performance\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_dir=\"./logs_cardio_qa\",\n",
        "    logging_steps=25,  # More frequent logging\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    disable_tqdm=False,\n",
        "    remove_unused_columns=True,\n",
        "    # T4-specific optimizations\n",
        "    gradient_checkpointing=False,  # Disabled for speed (T4 has enough memory)\n",
        "    optim=\"adamw_torch\",  # PyTorch AdamW is faster on T4\n",
        "    lr_scheduler_kwargs={\"num_cycles\": 0.5},  # Cosine annealing alternative\n",
        ")\n",
        "\n",
        "print(\"T4 GPU Configuration:\")\n",
        "print(f\"  - Batch Size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  - Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  - Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  - FP16 Enabled: {training_args.fp16}\")\n",
        "print(f\"  - Learning Rate: {training_args.learning_rate}\")\n",
        "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 9) Initialize Trainer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_qa_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready.\")\n",
        "\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RANDOM SEARCH HYPERPARAMETER TUNING (T4 GPU OPTIMIZED)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define hyperparameter search space for Random Search\n",
        "hp_search_space = {\n",
        "    \"epochs\": [3, 5, 7, 10],\n",
        "    \"lr\": [1e-5, 1.5e-5, 2e-5, 3e-5, 4e-5, 5e-5],\n",
        "    \"batch\": [8],  # Fixed for T4 memory constraints\n",
        "    \"warmup\": [0.1, 0.15, 0.2, 0.25],\n",
        "    \"weight_decay\": [0.0, 0.001, 0.01, 0.05],\n",
        "}\n",
        "\n",
        "# Random Search: Sample N random configurations\n",
        "N_RANDOM_CONFIGS = 10  # Adjust based on time budget\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "hyperparam_sets = []\n",
        "for _ in range(N_RANDOM_CONFIGS):\n",
        "    config = {\n",
        "        \"epochs\": int(np.random.choice(hp_search_space[\"epochs\"])),\n",
        "        \"lr\": float(np.random.choice(hp_search_space[\"lr\"])),\n",
        "        \"batch\": int(np.random.choice(hp_search_space[\"batch\"])),\n",
        "        \"warmup\": float(np.random.choice(hp_search_space[\"warmup\"])),\n",
        "        \"weight_decay\": float(np.random.choice(hp_search_space[\"weight_decay\"])),\n",
        "    }\n",
        "    hyperparam_sets.append(config)\n",
        "\n",
        "total_iters = len(hyperparam_sets)\n",
        "total_search_space = (len(hp_search_space[\"epochs\"]) * len(hp_search_space[\"lr\"]) *\n",
        "                      len(hp_search_space[\"warmup\"]) * len(hp_search_space[\"weight_decay\"]))\n",
        "coverage_percent = round(100 * total_iters / total_search_space, 2)\n",
        "\n",
        "print(f\"ðŸ” Random Search Strategy\")\n",
        "print(f\"Total search space size: {total_search_space} possible combinations\")\n",
        "print(f\"Random sampling: {total_iters} configurations ({coverage_percent}% coverage)\")\n",
        "print(f\"Fixed batch size: 8 (optimal for T4 16GB memory)\")\n",
        "print(f\"Gradient accumulation: 4 (effective batch size: 32)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Excel setup\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "ws.title = \"Random Search Results\"\n",
        "ws.append([\n",
        "    \"Iteration\", \"Epochs\", \"Learning Rate\", \"Batch Size\", \"Warmup Ratio\", \"Weight Decay\",\n",
        "    \"Accuracy\", \"F1-Score\", \"Precision\", \"Recall\", \"Runtime (s)\"\n",
        "])\n",
        "\n",
        "# Loop through each random configuration\n",
        "for i, params in enumerate(hyperparam_sets, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"â–¶ï¸ Random Search Config {i}/{total_iters}\")\n",
        "    print(f\"Params: Epochs={params['epochs']}, LR={params['lr']}, Warmup={params['warmup']}, Weight Decay={params['weight_decay']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # CRITICAL: Reload model from scratch for each iteration to avoid weight corruption\n",
        "    print(\"ðŸ”„ Reloading fresh model...\")\n",
        "    model_fresh = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model_fresh.to(device)\n",
        "\n",
        "    # Update training arguments dynamically\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_random_{i}\",\n",
        "        num_train_epochs=params[\"epochs\"],\n",
        "        per_device_train_batch_size=params[\"batch\"],\n",
        "        per_device_eval_batch_size=params[\"batch\"],\n",
        "        gradient_accumulation_steps=4,  # Fixed at 4 for T4 optimization\n",
        "        learning_rate=params[\"lr\"],\n",
        "        warmup_ratio=params[\"warmup\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_dir=f\"./logs_random_{i}\",\n",
        "        report_to=[],\n",
        "        disable_tqdm=True,\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        optim=\"adamw_torch\",  # T4-optimized optimizer\n",
        "        max_grad_norm=1.0,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model_fresh,  # Use fresh model instance\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "        compute_metrics=compute_qa_metrics,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    runtime = round(time.time() - start_time, 2)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Extract basic metrics (handle 'eval_' prefix from HF evaluate)\n",
        "    exact_match = eval_results.get(\"eval_exact_match\", eval_results.get(\"exact_match\", 0))\n",
        "    start_acc = eval_results.get(\"eval_start_accuracy\", eval_results.get(\"start_accuracy\", 0))\n",
        "    end_acc = eval_results.get(\"eval_end_accuracy\", eval_results.get(\"end_accuracy\", 0))\n",
        "    accuracy = (exact_match + start_acc + end_acc) / 3\n",
        "    f1 = eval_results.get(\"eval_f1\", eval_results.get(\"f1\", 0))\n",
        "\n",
        "    # Calculate REAL precision and recall from token overlap\n",
        "    predictions = trainer.predict(tokenized_eval)\n",
        "    pred_starts = np.argmax(predictions.predictions[0], axis=1)\n",
        "    pred_ends = np.argmax(predictions.predictions[1], axis=1)\n",
        "\n",
        "    true_starts = np.asarray(predictions.label_ids[0]).reshape(-1)\n",
        "    true_ends = np.asarray(predictions.label_ids[1]).reshape(-1)\n",
        "\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for ps, pe, ts, te in zip(pred_starts, pred_ends, true_starts, true_ends):\n",
        "        ps, pe, ts, te = int(ps), int(pe), int(ts), int(te)\n",
        "        pred_tokens = set(range(ps, pe + 1))\n",
        "        true_tokens = set(range(ts, te + 1))\n",
        "\n",
        "        if not pred_tokens and not true_tokens:\n",
        "            precision_scores.append(1.0)\n",
        "            recall_scores.append(1.0)\n",
        "        elif not pred_tokens or not true_tokens:\n",
        "            precision_scores.append(0.0)\n",
        "            recall_scores.append(0.0)\n",
        "        else:\n",
        "            common = len(pred_tokens & true_tokens)\n",
        "            if common == 0:\n",
        "                precision_scores.append(0.0)\n",
        "                recall_scores.append(0.0)\n",
        "            else:\n",
        "                precision = common / len(pred_tokens)\n",
        "                recall = common / len(true_tokens)\n",
        "                precision_scores.append(precision)\n",
        "                recall_scores.append(recall)\n",
        "\n",
        "    precision = float(np.mean(precision_scores))\n",
        "    recall = float(np.mean(recall_scores))\n",
        "\n",
        "    ws.append([\n",
        "        i, params[\"epochs\"], params[\"lr\"], params[\"batch\"], params[\"warmup\"], params[\"weight_decay\"],\n",
        "        round(accuracy, 4), round(f1, 4), round(precision, 4), round(recall, 4), runtime\n",
        "    ])\n",
        "\n",
        "\n",
        "    print(f\"âœ… Iteration {i} done â€” F1: {f1:.4f}, Accuracy: {accuracy:.4f}, Time: {runtime}s\")\n",
        "\n",
        "# ==========================\n",
        "# âœ… Save All Results in One Excel File\n",
        "# ==========================\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Auto-format headers for readability\n",
        "for cell in ws[1]:\n",
        "    cell.font = Font(bold=True)\n",
        "    cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "# Adjust column widths (optional aesthetic)\n",
        "for col in ws.columns:\n",
        "    max_length = 0\n",
        "    col_letter = col[0].column_letter\n",
        "    for cell in col:\n",
        "        try:\n",
        "            if len(str(cell.value)) > max_length:\n",
        "                max_length = len(str(cell.value))\n",
        "        except:\n",
        "            pass\n",
        "    adjusted_width = (max_length + 2)\n",
        "    ws.column_dimensions[col_letter].width = adjusted_width\n",
        "\n",
        "# Save Excel file\n",
        "output_excel = \"/content/BioBERT_Random_Search_Results.xlsx\"\n",
        "wb.save(output_excel)\n",
        "\n",
        "print(f\"\\nâœ… All {total_iters} random search runs completed successfully!\")\n",
        "print(\"ðŸ“Š Final results saved in one Excel file:\")\n",
        "print(f\"âž¡ï¸ {output_excel}\")\n",
        "print(f\"\\nðŸ” Random Search Summary:\")\n",
        "print(f\"   - Total search space: {total_search_space} combinations\")\n",
        "print(f\"   - Explored: {total_iters} configurations ({coverage_percent}% coverage)\")\n",
        "print(f\"   - Advantage: More efficient exploration than Grid Search\")\n",
        "\n",
        "# 10) Train\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nðŸš€ Training in progress...\\n\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Loss:\", getattr(train_result, \"training_loss\", None))\n",
        "\n",
        "# 11) Evaluate\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "for k, v in sorted(eval_results.items()):\n",
        "    print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ANALYZE AND DISPLAY BEST HYPERPARAMETER CONFIGURATION\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEST HYPERPARAMETER CONFIGURATION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Read the Excel file with results\n",
        "import pandas as pd\n",
        "results_df = pd.read_excel(\"/content/BioBERT_Random_Search_Results.xlsx\")\n",
        "\n",
        "# Display all results sorted by F1-Score (descending)\n",
        "print(\"\\nAll Configurations Ranked by F1-Score:\")\n",
        "print(\"=\" * 60)\n",
        "results_sorted = results_df.sort_values(\"F1-Score\", ascending=False)\n",
        "print(results_sorted.to_string(index=False))\n",
        "\n",
        "# Find best configuration by F1-Score\n",
        "best_idx = results_df[\"F1-Score\"].idxmax()\n",
        "best_config = results_df.iloc[best_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" BEST HYPERPARAMETER CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBest Configuration (Iteration {int(best_config['Iteration'])}):\")\n",
        "print(f\"   - Epochs:        {int(best_config['Epochs'])}\")\n",
        "print(f\"   - Learning Rate: {best_config['Learning Rate']:.2e}\")\n",
        "print(f\"   - Batch Size:    {int(best_config['Batch Size'])}\")\n",
        "print(f\"   - Warmup Ratio:  {best_config['Warmup Ratio']:.2f}\")\n",
        "print(f\"   - Weight Decay:  {best_config['Weight Decay']:.4f}\")\n",
        "print(f\"\\ Performance Metrics:\")\n",
        "print(f\"   - F1-Score:      {best_config['F1-Score']:.4f}\")\n",
        "print(f\"   - Accuracy:      {best_config['Accuracy']:.4f}\")\n",
        "print(f\"   - Precision:     {best_config['Precision']:.4f}\")\n",
        "print(f\"   - Recall:        {best_config['Recall']:.4f}\")\n",
        "print(f\"   - Runtime:       {best_config['Runtime (s)']:.2f} seconds\")\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" INSIGHTS FROM RANDOM SEARCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Best vs Average performance\n",
        "avg_f1 = results_df[\"F1-Score\"].mean()\n",
        "improvement = ((best_config['F1-Score'] - avg_f1) / avg_f1) * 100\n",
        "\n",
        "print(f\"\\nAverage F1-Score across all configs: {avg_f1:.4f}\")\n",
        "print(f\"Best F1-Score improvement over average: +{improvement:.2f}%\")\n",
        "\n",
        "# Top 3 configurations\n",
        "print(\"\\n Top 3 Configurations by F1-Score:\")\n",
        "top3 = results_sorted.head(3)\n",
        "for idx, row in top3.iterrows():\n",
        "    print(f\"\\n{[list(top3.index).index(idx)]} Rank {list(top3.index).index(idx) + 1}:\")\n",
        "    print(f\"   Iteration {int(row['Iteration'])} | F1: {row['F1-Score']:.4f} | \"\n",
        "          f\"LR: {row['Learning Rate']:.2e} | Epochs: {int(row['Epochs'])} | \"\n",
        "          f\"Warmup: {row['Warmup Ratio']:.2f}\")\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\n Hyperparameter Impact Analysis:\")\n",
        "correlations = results_df[['Epochs', 'Learning Rate', 'Warmup Ratio', 'Weight Decay', 'F1-Score']].corr()['F1-Score'].drop('F1-Score')\n",
        "print(\"\\nCorrelation with F1-Score:\")\n",
        "for param, corr in correlations.sort_values(ascending=False).items():\n",
        "    direction = \"â†‘ Positive\" if corr > 0 else \"â†“ Negative\"\n",
        "    print(f\"   {param:15s}: {corr:+.3f} ({direction})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QBhQeiMx5bA",
        "outputId": "4e75e1ab-f4c3-4035-ec5b-57a7080f81ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "BEST HYPERPARAMETER CONFIGURATION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "All Configurations Ranked by F1-Score:\n",
            "============================================================\n",
            " Iteration  Epochs  Learning Rate  Batch Size  Warmup Ratio  Weight Decay  Accuracy  F1-Score  Precision  Recall  Runtime (s)\n",
            "         4       7       0.000040           8          0.25         0.050    0.5825    0.9896     0.9873  0.9925       159.92\n",
            "         5      10       0.000020           8          0.15         0.000    0.5146    0.9822     0.9784  0.9866       227.19\n",
            "         1       7       0.000030           8          0.10         0.010    0.4854    0.9756     0.9761  0.9805       160.72\n",
            "         2       7       0.000040           8          0.10         0.010    0.4693    0.9694     0.9766  0.9724       159.83\n",
            "         6       5       0.000030           8          0.15         0.001    0.4401    0.9640     0.9575  0.9721       114.38\n",
            "         3       5       0.000020           8          0.20         0.010    0.4142    0.9628     0.9596  0.9676       114.62\n",
            "         9      10       0.000010           8          0.10         0.010    0.4531    0.9618     0.9563  0.9686       228.50\n",
            "        10       7       0.000015           8          0.25         0.050    0.4628    0.9582     0.9554  0.9620       160.25\n",
            "         8      10       0.000015           8          0.15         0.000    0.6440    0.9528     0.9538  0.9568       227.16\n",
            "         7       5       0.000030           8          0.10         0.000    0.4660    0.9470     0.9435  0.9563       113.81\n",
            "\n",
            "============================================================\n",
            " BEST HYPERPARAMETER CONFIGURATION\n",
            "============================================================\n",
            "\n",
            "Best Configuration (Iteration 4):\n",
            "   - Epochs:        7\n",
            "   - Learning Rate: 4.00e-05\n",
            "   - Batch Size:    8\n",
            "   - Warmup Ratio:  0.25\n",
            "   - Weight Decay:  0.0500\n",
            "\\ Performance Metrics:\n",
            "   - F1-Score:      0.9896\n",
            "   - Accuracy:      0.5825\n",
            "   - Precision:     0.9873\n",
            "   - Recall:        0.9925\n",
            "   - Runtime:       159.92 seconds\n",
            "\n",
            "============================================================\n",
            " INSIGHTS FROM RANDOM SEARCH\n",
            "============================================================\n",
            "\n",
            "Average F1-Score across all configs: 0.9663\n",
            "Best F1-Score improvement over average: +2.41%\n",
            "\n",
            " Top 3 Configurations by F1-Score:\n",
            "\n",
            "[0] Rank 1:\n",
            "   Iteration 4 | F1: 0.9896 | LR: 4.00e-05 | Epochs: 7 | Warmup: 0.25\n",
            "\n",
            "[1] Rank 2:\n",
            "   Iteration 5 | F1: 0.9822 | LR: 2.00e-05 | Epochs: 10 | Warmup: 0.15\n",
            "\n",
            "[2] Rank 3:\n",
            "   Iteration 1 | F1: 0.9756 | LR: 3.00e-05 | Epochs: 7 | Warmup: 0.10\n",
            "\n",
            " Hyperparameter Impact Analysis:\n",
            "\n",
            "Correlation with F1-Score:\n",
            "   Learning Rate  : +0.447 (â†‘ Positive)\n",
            "   Weight Decay   : +0.346 (â†‘ Positive)\n",
            "   Warmup Ratio   : +0.271 (â†‘ Positive)\n",
            "   Epochs         : +0.180 (â†‘ Positive)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) Save the trained model\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define output directory\n",
        "output_model_dir = \"./fine_tuned_cardio_qa_model\"\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model(output_model_dir)\n",
        "tokenizer.save_pretrained(output_model_dir)\n",
        "\n",
        "print(f\"âœ… Model saved to: {output_model_dir}\")\n",
        "print(f\"ðŸ“¦ Saved components:\")\n",
        "print(f\"   - Model weights: pytorch_model.bin\")\n",
        "print(f\"   - Model config: config.json\")\n",
        "print(f\"   - Tokenizer files: tokenizer_config.json, vocab.txt, etc.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Optional: Save to Google Drive for persistence\n",
        "SAVE_TO_DRIVE = False  # Set to True if you want to save to Drive\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        import shutil\n",
        "\n",
        "        # Mount Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Define Drive destination\n",
        "        drive_model_dir = \"/content/drive/MyDrive/fine_tuned_cardio_qa_model\"\n",
        "\n",
        "        # Copy model to Drive\n",
        "        print(f\"\\nðŸ“¤ Copying model to Google Drive...\")\n",
        "        if os.path.exists(drive_model_dir):\n",
        "            shutil.rmtree(drive_model_dir)\n",
        "        shutil.copytree(output_model_dir, drive_model_dir)\n",
        "\n",
        "        print(f\"âœ… Model also saved to Google Drive: {drive_model_dir}\")\n",
        "        print(\"ðŸ’¾ Your model will persist even after the Colab session ends!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not save to Google Drive: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸŽ‰ TRAINING PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTIBZkPJi05L",
        "outputId": "f6330341-20a7-41d6-f851-8aa1cfc5b5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING TRAINED MODEL\n",
            "============================================================\n",
            "âœ… Model saved to: ./fine_tuned_cardio_qa_model\n",
            "ðŸ“¦ Saved components:\n",
            "   - Model weights: pytorch_model.bin\n",
            "   - Model config: config.json\n",
            "   - Tokenizer files: tokenizer_config.json, vocab.txt, etc.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "ðŸŽ‰ TRAINING PIPELINE COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4UFad56i6ma",
        "outputId": "7efb7829-0e93-4966-f1ae-574087347e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}