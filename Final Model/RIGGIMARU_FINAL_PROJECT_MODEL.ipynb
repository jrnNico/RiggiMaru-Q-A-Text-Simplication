{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe15106e083647a0a284d46da5f6c34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dd270ecc4cc485482a20dec385e2e1b",
              "IPY_MODEL_92c5b74d1c5c4297aaeab2f3138b411a",
              "IPY_MODEL_5aa36bf7fbf844dc9b987713123eee72"
            ],
            "layout": "IPY_MODEL_a132da3734454fcaa2c3986896c0db7d"
          }
        },
        "1dd270ecc4cc485482a20dec385e2e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4be7009abf6b4800ae9750c18defbb41",
            "placeholder": "​",
            "style": "IPY_MODEL_c62476b5c0e44e7c90c9b071632938c3",
            "value": "Tokenizing train: 100%"
          }
        },
        "92c5b74d1c5c4297aaeab2f3138b411a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d64c0db8871464fa8cbfd96c0233dc6",
            "max": 555,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da957cb516d345039fb15c5085109d28",
            "value": 555
          }
        },
        "5aa36bf7fbf844dc9b987713123eee72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c4569dae5c4c5abaf6f8e5c7b82803",
            "placeholder": "​",
            "style": "IPY_MODEL_3f997311ecd8430187ccef95d0c21bf9",
            "value": " 555/555 [00:01&lt;00:00, 358.46 examples/s]"
          }
        },
        "a132da3734454fcaa2c3986896c0db7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be7009abf6b4800ae9750c18defbb41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62476b5c0e44e7c90c9b071632938c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d64c0db8871464fa8cbfd96c0233dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da957cb516d345039fb15c5085109d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74c4569dae5c4c5abaf6f8e5c7b82803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f997311ecd8430187ccef95d0c21bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df69ebc03a9f49efa44497dc2144e7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69e81c1c6b0945ff8d31d398db02630a",
              "IPY_MODEL_fd75e6a2958648dd8659c437867388a3",
              "IPY_MODEL_7d8d855e5a5d430da832386fdb628e6c"
            ],
            "layout": "IPY_MODEL_29f480ddfbb54f5c995372a41a103b61"
          }
        },
        "69e81c1c6b0945ff8d31d398db02630a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d69572b05f4c60b526afb42cdb5a6c",
            "placeholder": "​",
            "style": "IPY_MODEL_35d5b339609741a8a55ac35c0ace3822",
            "value": "Tokenizing eval: 100%"
          }
        },
        "fd75e6a2958648dd8659c437867388a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91508b106b7a4205a158dc983e34dba3",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ee0d166148c4d07a07febdda75e049b",
            "value": 99
          }
        },
        "7d8d855e5a5d430da832386fdb628e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e76cc733b342b18d353dc2314da9de",
            "placeholder": "​",
            "style": "IPY_MODEL_8dc244350c3d443c992f557e1a2cdb48",
            "value": " 99/99 [00:00&lt;00:00, 273.23 examples/s]"
          }
        },
        "29f480ddfbb54f5c995372a41a103b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d69572b05f4c60b526afb42cdb5a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d5b339609741a8a55ac35c0ace3822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91508b106b7a4205a158dc983e34dba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee0d166148c4d07a07febdda75e049b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e76cc733b342b18d353dc2314da9de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dc244350c3d443c992f557e1a2cdb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af66a22ccdb44bdb0b60e99941ca3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2ef662430f254154ae2428c5ceebef2e",
            "placeholder": "Enter your cardiovascular health question here...",
            "rows": null,
            "style": "IPY_MODEL_b7b6bb1a60ba4623b4c44cf2c742bed6",
            "value": "What causes Heart Murmur ?"
          }
        },
        "2ef662430f254154ae2428c5ceebef2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "80px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b7b6bb1a60ba4623b4c44cf2c742bed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "100px"
          }
        },
        "27f330795e354c84a78ba2955d710564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Simplify answer with FLAN-T5",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9477effb4544414a9780e6dd375f0fe1",
            "style": "IPY_MODEL_08563c8c8f1d4f329d07de755a88901e",
            "value": true
          }
        },
        "9477effb4544414a9780e6dd375f0fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08563c8c8f1d4f329d07de755a88901e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "a25d3835bf8e462986ba62ed1af7ce04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Get Answer",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_2eab4395213f4fd796d8a4994f2df19e",
            "style": "IPY_MODEL_cc5248289e324300ac056f966b15011a",
            "tooltip": "Click to get answer"
          }
        },
        "2eab4395213f4fd796d8a4994f2df19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "40px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "150px"
          }
        },
        "cc5248289e324300ac056f966b15011a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4f2bcc8723284e6086cf85c96c9e2a79": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_15f26723f76546c29a867c92e953348d",
            "msg_id": "",
            "outputs": []
          }
        },
        "15f26723f76546c29a867c92e953348d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cardiovascular Q&A System using BlueBERT and FLAN-T5\n",
        "\n",
        "## Environment Setup and Model Training\n",
        "\n",
        "This code cell sets up the complete environment needed to train a cardiovascular question-answering system. I started by installing the essential libraries like `transformers`, `datasets`, and `accelerate`, which are the backbone of working with modern language models. After installation, the code performs a quick check to confirm that PyTorch is available and whether a GPU is detected, which is crucial since training these models on a CPU would take significantly longer.\n",
        "\n",
        "Once the environment is verified, the code imports all the necessary libraries for data handling and model training. I used `AutoTokenizer` and `AutoModelForQuestionAnswering` from the transformers library to load the pre-trained BlueBERT model specifically fine-tuned for medical questions. The GPU configuration step assigns the model to CUDA if available, ensuring faster training times.\n",
        "\n",
        "The dataset loading section handles file uploads in Google Colab, either through Google Drive mounting or direct file upload. After loading the cardiovascular dataset from a CSV file, the code performs basic preprocessing by removing any null values and splitting the data into training and validation sets with an 85/15 ratio. This split allows the model to learn from most of the data while keeping some aside to evaluate how well it generalizes.\n",
        "\n",
        "Tokenization is where the text gets converted into a format the model can understand. I set the maximum token length to 384 with a stride of 128, which means the model can handle longer contexts by creating overlapping windows. The `prepare_train_features` function tokenizes both questions and answers, then identifies the start and end positions of answers within the context, which is exactly what the question-answering model needs during training.\n",
        "\n",
        "For evaluation, I implemented custom metrics that calculate exact match accuracy, start and end position accuracy, and F1 scores. These metrics help measure how precisely the model can identify answer spans within the text. The training configuration uses the best hyperparameters discovered from previous random search experiments: 7 epochs, a learning rate of 4e-5, batch size of 8, warmup ratio of 0.10, and weight decay of 0.01. These specific values were chosen because they achieved the highest F1-score of 0.9972 during hyperparameter tuning.\n",
        "\n",
        "The Trainer class from Hugging Face brings everything together, handling the actual training loop, evaluation during each epoch, and saving the best performing model. After training completes, the model is saved locally so it can be reused without retraining, which saves considerable time and computational resources."
      ],
      "metadata": {
        "id": "IzczsTwV1NNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe15106e083647a0a284d46da5f6c34a",
            "1dd270ecc4cc485482a20dec385e2e1b",
            "92c5b74d1c5c4297aaeab2f3138b411a",
            "5aa36bf7fbf844dc9b987713123eee72",
            "a132da3734454fcaa2c3986896c0db7d",
            "4be7009abf6b4800ae9750c18defbb41",
            "c62476b5c0e44e7c90c9b071632938c3",
            "6d64c0db8871464fa8cbfd96c0233dc6",
            "da957cb516d345039fb15c5085109d28",
            "74c4569dae5c4c5abaf6f8e5c7b82803",
            "3f997311ecd8430187ccef95d0c21bf9",
            "df69ebc03a9f49efa44497dc2144e7f6",
            "69e81c1c6b0945ff8d31d398db02630a",
            "fd75e6a2958648dd8659c437867388a3",
            "7d8d855e5a5d430da832386fdb628e6c",
            "29f480ddfbb54f5c995372a41a103b61",
            "e5d69572b05f4c60b526afb42cdb5a6c",
            "35d5b339609741a8a55ac35c0ace3822",
            "91508b106b7a4205a158dc983e34dba3",
            "2ee0d166148c4d07a07febdda75e049b",
            "d9e76cc733b342b18d353dc2314da9de",
            "8dc244350c3d443c992f557e1a2cdb48"
          ]
        },
        "id": "IzEucfJ_f2My",
        "outputId": "d3f00309-f0dd-45a3-cf6e-c06535c6f4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ENVIRONMENT\n",
            "============================================================\n",
            "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "PyTorch: 2.8.0+cu126\n",
            "GPU: Tesla T4 | CUDA: 12.6\n",
            "============================================================\n",
            "============================================================\n",
            "GPU CONFIGURATION\n",
            "============================================================\n",
            "GPU Available: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.74 GB\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c184f6a4-4e52-44ef-b2bd-e7044529645a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c184f6a4-4e52-44ef-b2bd-e7044529645a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving medquadCardiovascular.csv to medquadCardiovascular (2).csv\n",
            "Dataset CSV: medquadCardiovascular (2).csv\n",
            "\n",
            "============================================================\n",
            "LOADING CARDIOVASCULAR DATASET\n",
            "============================================================\n",
            "Total records: 654\n",
            "Columns: ['question', 'answer', 'source', 'focus_area']\n",
            "Sample question: What is (are) High Blood Pressure ?...\n",
            "Sample answer chars: 5586\n",
            "Train: 555 | Val: 99\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL AND TOKENIZER\n",
            "============================================================\n",
            "Model: aaditya/Bluebert_emrqa\n",
            "Model loaded.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TOKENIZING DATASET\n",
            "============================================================\n",
            "Tokenizing train...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train:   0%|          | 0/555 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe15106e083647a0a284d46da5f6c34a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing eval...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval:   0%|          | 0/99 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df69ebc03a9f49efa44497dc2144e7f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "============================================================\n",
            "Metrics ready.\n",
            "\n",
            "============================================================\n",
            "TRAINING CONFIGURATION - BEST HYPERPARAMETERS\n",
            "============================================================\n",
            "Using best configuration from Random Search (Iteration 2):\n",
            "  - Epochs: 7\n",
            "  - Learning Rate: 4e-5\n",
            "  - Batch Size: 8\n",
            "  - Warmup Ratio: 0.10\n",
            "  - Weight Decay: 0.01\n",
            "  - Expected F1-Score: 0.9972\n",
            "============================================================\n",
            "Configuration set with best hyperparameters.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INITIALIZING TRAINER\n",
            "============================================================\n",
            "Trainer ready with best hyperparameters.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING WITH BEST HYPERPARAMETERS\n",
            "============================================================\n",
            "\n",
            " Training in progress...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [231/231 04:28, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>Start Accuracy</th>\n",
              "      <th>End Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.951200</td>\n",
              "      <td>1.754984</td>\n",
              "      <td>0.106061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.106061</td>\n",
              "      <td>0.900393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.727300</td>\n",
              "      <td>1.069500</td>\n",
              "      <td>0.196970</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.196970</td>\n",
              "      <td>0.982799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.018800</td>\n",
              "      <td>0.555065</td>\n",
              "      <td>0.550505</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.550505</td>\n",
              "      <td>0.991159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.519500</td>\n",
              "      <td>0.429502</td>\n",
              "      <td>0.656566</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.656566</td>\n",
              "      <td>0.995524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.402064</td>\n",
              "      <td>0.707071</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.707071</td>\n",
              "      <td>0.996427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.331300</td>\n",
              "      <td>0.391365</td>\n",
              "      <td>0.752525</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.752525</td>\n",
              "      <td>0.996906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.219400</td>\n",
              "      <td>0.314917</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.997239</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING COMPLETED\n",
            "============================================================\n",
            "Training Loss: 1.6457947626774445\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 7.0\n",
            "eval_end_accuracy: 0.7878787878787878\n",
            "eval_exact_match: 0.7878787878787878\n",
            "eval_f1: 0.9972390284151159\n",
            "eval_loss: 0.3149166405200958\n",
            "eval_runtime: 1.4394\n",
            "eval_samples_per_second: 137.556\n",
            "eval_start_accuracy: 1.0\n",
            "eval_steps_per_second: 17.368\n",
            "\n",
            "============================================================\n",
            "SAVING BEST MODEL\n",
            "============================================================\n",
            "Best model saved to './best_cardio_qa_model'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 1) Environment setup (Colab)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(packages):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n",
        "\n",
        "# Core ML stack\n",
        "pip_install([\n",
        "    \"transformers>=4.44.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.26.0\",\n",
        "    \"evaluate>=0.4.0\",\n",
        "])\n",
        "\n",
        "# Colab-specific checks\n",
        "try:\n",
        "    import torch\n",
        "    import platform\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ENVIRONMENT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Python: {sys.version.split()[0]} | Platform: {platform.platform()}\")\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"GPU not detected. Enable a GPU in Runtime > Change runtime type > T4/other.\")\n",
        "    print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(\"Environment check failed:\", e)\n",
        "\n",
        "# 2) Imports and GPU config\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import default_data_collator\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2)} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available; training will be slower.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 3) Dataset loading (Cardiovascular QA)\n",
        "# Option A: Mount Drive\n",
        "USE_DRIVE = False  # set True to use Drive\n",
        "CSV_PATH = \"\"       # e.g., \"/content/drive/MyDrive/medquadCardiovascular.csv\"\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Option B: Upload a file\n",
        "USE_UPLOAD = not USE_DRIVE\n",
        "if USE_UPLOAD:\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        uploaded = files.upload()\n",
        "        # Pick the first uploaded file\n",
        "        if uploaded:\n",
        "            CSV_PATH = list(uploaded.keys())[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not CSV_PATH:\n",
        "    # Fallback sample: you can place the CSV at a public URL and download it\n",
        "    # For now, raise an error to prompt the user.\n",
        "    raise ValueError(\"Please provide CSV_PATH via Drive or upload.\")\n",
        "\n",
        "print(\"Dataset CSV:\", CSV_PATH)\n",
        "\n",
        "# 4) Data Loading and Preprocessing\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING CARDIOVASCULAR DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset = pd.read_csv(CSV_PATH)\n",
        "print(f\"Total records: {len(dataset)}\")\n",
        "print(f\"Columns: {list(dataset.columns)}\")\n",
        "print(f\"Sample question: {str(dataset.iloc[0]['question'])[:80]}...\")\n",
        "print(f\"Sample answer chars: {len(str(dataset.iloc[0]['answer']))}\")\n",
        "\n",
        "# Drop nulls\n",
        "dataset = dataset.dropna(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n",
        "\n",
        "# Train/val split (85/15)\n",
        "dataset_shuffled = dataset.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "split_idx = int(len(dataset_shuffled) * 0.85)\n",
        "train_data = dataset_shuffled.iloc[:split_idx].copy()\n",
        "eval_data = dataset_shuffled.iloc[split_idx:].copy()\n",
        "\n",
        "print(f\"Train: {len(train_data)} | Val: {len(eval_data)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 5) Model and tokenizer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING MODEL AND TOKENIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "MODEL_NAME = \"aaditya/Bluebert_emrqa\"\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# 6) Tokenization and Feature Preparation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOKENIZING DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_data)\n",
        "eval_ds = Dataset.from_pandas(eval_data)\n",
        "\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"answer\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "\n",
        "        context_start = None\n",
        "        context_end = None\n",
        "        for idx, seq_id in enumerate(sequence_ids):\n",
        "            if seq_id == 1:\n",
        "                if context_start is None:\n",
        "                    context_start = idx\n",
        "                context_end = idx\n",
        "\n",
        "        if context_start is None:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            answer_start = context_start\n",
        "            answer_end = min(context_start + 50, context_end)\n",
        "            start_positions.append(answer_start)\n",
        "            end_positions.append(answer_end)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "\n",
        "    # Drop offset_mapping so it isn't fed to the model\n",
        "    if \"offset_mapping\" in tokenized:\n",
        "        tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing train...\")\n",
        "tokenized_train = train_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        "    desc=\"Tokenizing train\",\n",
        ")\n",
        "\n",
        "print(\"Tokenizing eval...\")\n",
        "tokenized_eval = eval_ds.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=eval_ds.column_names,\n",
        "    desc=\"Tokenizing eval\",\n",
        ")\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 7) Evaluation Metrics\n",
        "import numpy as np\n",
        "\n",
        "def compute_qa_metrics(eval_pred):\n",
        "    predictions, label_ids = eval_pred\n",
        "    start_logits, end_logits = predictions\n",
        "\n",
        "    pred_starts = np.argmax(start_logits, axis=1)\n",
        "    pred_ends = np.argmax(end_logits, axis=1)\n",
        "\n",
        "    # Ensure label_ids is treated as a tuple\n",
        "    true_starts = np.asarray(label_ids[0]).reshape(-1)\n",
        "    true_ends = np.asarray(label_ids[1]).reshape(-1)\n",
        "\n",
        "    exact_match = np.mean((pred_starts == true_starts) & (pred_ends == true_ends))\n",
        "    start_accuracy = np.mean(pred_starts == true_starts)\n",
        "    end_accuracy = np.mean(pred_ends == true_ends)\n",
        "\n",
        "    f1_scores = []\n",
        "    for ps, pe, ts, te in zip(pred_starts, pred_ends, true_starts, true_ends):\n",
        "        ps, pe, ts, te = int(ps), int(pe), int(ts), int(te)\n",
        "        pred_tokens = set(range(ps, pe + 1))\n",
        "        true_tokens = set(range(ts, te + 1))\n",
        "        if not pred_tokens and not true_tokens:\n",
        "            f1_scores.append(1.0)\n",
        "        elif not pred_tokens or not true_tokens:\n",
        "            f1_scores.append(0.0)\n",
        "        else:\n",
        "            common = len(pred_tokens & true_tokens)\n",
        "            if common == 0:\n",
        "                f1_scores.append(0.0)\n",
        "            else:\n",
        "                precision = common / len(pred_tokens)\n",
        "                recall = common / len(true_tokens)\n",
        "                f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": float(exact_match),\n",
        "        \"start_accuracy\": float(start_accuracy),\n",
        "        \"end_accuracy\": float(end_accuracy),\n",
        "        \"f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
        "    }\n",
        "\n",
        "print(\"Metrics ready.\")\n",
        "\n",
        "# 8) OPTIMIZED Training Configuration - Best Hyperparameters from Iteration 2\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING CONFIGURATION - BEST HYPERPARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Using best configuration from Random Search (Iteration 2):\")\n",
        "print(\"  - Epochs: 7\")\n",
        "print(\"  - Learning Rate: 4e-5\")\n",
        "print(\"  - Batch Size: 8\")\n",
        "print(\"  - Warmup Ratio: 0.10\")\n",
        "print(\"  - Weight Decay: 0.01\")\n",
        "print(\"  - Expected F1-Score: 0.9972\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_best_model\",\n",
        "    num_train_epochs=7,  # Best from Iteration 2\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=4e-5,  # Best from Iteration 2\n",
        "    weight_decay=0.01,  # Best from Iteration 2\n",
        "    warmup_ratio=0.10,  # Best from Iteration 2\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_dir=\"./logs_best_model\",\n",
        "    logging_steps=25,\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        "    disable_tqdm=False,\n",
        "    remove_unused_columns=True,\n",
        "    gradient_checkpointing=False,\n",
        "    optim=\"adamw_torch\",\n",
        ")\n",
        "\n",
        "print(\"Configuration set with best hyperparameters.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 9) Initialize Trainer\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_qa_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready with best hyperparameters.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 10) Train with Best Hyperparameters\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING WITH BEST HYPERPARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n Training in progress...\\n\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Loss:\", getattr(train_result, \"training_loss\", None))\n",
        "\n",
        "# 11) Evaluate\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "for k, v in sorted(eval_results.items()):\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# 12) Save the Best Model\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING BEST MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model.save_pretrained(\"./best_cardio_qa_model\")\n",
        "tokenizer.save_pretrained(\"./best_cardio_qa_model\")\n",
        "\n",
        "print(\"Best model saved to './best_cardio_qa_model'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading FLAN-T5 for Answer Simplification\n",
        "\n",
        "This code cell integrates FLAN-T5-Base, a text-to-text model designed for language understanding and generation tasks. The purpose here is to take the technical medical answers produced by BlueBERT and convert them into simpler, more patient-friendly language that's easier to understand.\n",
        "\n",
        "The code starts by importing the necessary classes for sequence-to-sequence models, which are different from the question-answering model used earlier. I loaded the `google/flan-t5-base` model along with its tokenizer and moved it to the GPU for faster inference. FLAN-T5 is particularly good at following instructions, so when given a prompt like \"Simplify this medical answer,\" it can rephrase complex medical terminology into everyday language.\n",
        "\n",
        "The `simplify_answer` function takes a medical answer as input and constructs a prompt that instructs the model to simplify it. The tokenizer converts this prompt into tokens, and the model generates a simplified version using beam search with specific parameters like `num_beams=4` and `temperature=0.7` to balance between creativity and accuracy. The `max_length` parameter is set to 300 tokens, giving the model enough room to produce complete, helpful explanations rather than cutting off mid-sentence.\n",
        "\n",
        "What I found interesting is that by adjusting parameters like `repetition_penalty` and `length_penalty`, the simplified answers feel more natural and avoid repeating the same phrases. The function returns the decoded simplified text, which can then be displayed alongside the original BlueBERT answer, giving users both technical accuracy and accessible explanations."
      ],
      "metadata": {
        "id": "0LJPC9671ZCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD FLAN-T5 FOR ANSWER SIMPLIFICATION\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING FLAN-T5 FOR ANSWER SIMPLIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer as T5Tokenizer\n",
        "\n",
        "# Load FLAN-T5-Base for simplification\n",
        "flan_model_name = \"google/flan-t5-base\"\n",
        "print(f\"Loading {flan_model_name}...\")\n",
        "\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(flan_model_name)\n",
        "flan_model = AutoModelForSeq2SeqLM.from_pretrained(flan_model_name)\n",
        "flan_model.to(device)\n",
        "\n",
        "print(\"FLAN-T5-Base loaded successfully!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def simplify_answer(answer_text, max_length=300):\n",
        "    \"\"\"\n",
        "    Simplify medical answer using FLAN-T5 to make it patient-friendly\n",
        "    \"\"\"\n",
        "    # Enhanced prompt with specific instructions\n",
        "    prompt = f\"\"\"Explain this medical information in simple terms that a patient can understand.\n",
        "Use everyday language and avoid medical jargon:\n",
        "\n",
        "{answer_text}\n",
        "\n",
        "Simple explanation:\"\"\"\n",
        "\n",
        "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    outputs = flan_model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,  # Increased from 200 to 300\n",
        "        min_length=30,          # Increased from 20 to 30\n",
        "        num_beams=5,            # Increased from 4 to 5 for better quality\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0, # Reduced from 2.5 to 2.0 for more natural text\n",
        "        length_penalty=1.0,     # Reduced from 1.2 to 1.0\n",
        "        temperature=0.7,        # Added temperature for more natural output\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    simplified = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return simplified.strip()\n",
        "\n",
        "    # If the simplified answer is too short or just repeats input, provide a generic explanation\n",
        "    if len(simplified.strip()) < 15 or simplified.lower().strip() in answer_text.lower():\n",
        "        # Fallback: clean up the original answer\n",
        "        simplified = answer_text.replace('-', '').strip()\n",
        "        if len(simplified) > 2000:\n",
        "            simplified = simplified[:2000] + \"...\"\n",
        "\n",
        "    return simplified\n",
        "\n",
        "print(\"\\nAnswer simplification function ready!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atF13stAhbtQ",
        "outputId": "72a9fa9e-621f-4ce9-82db-d53bab030a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LOADING FLAN-T5 FOR ANSWER SIMPLIFICATION\n",
            "============================================================\n",
            "Loading google/flan-t5-base...\n",
            "FLAN-T5-Base loaded successfully!\n",
            "============================================================\n",
            "\n",
            "Answer simplification function ready!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Q&A System with Real Data\n",
        "\n",
        "This code cell creates the infrastructure for testing the cardiovascular Q&A system with actual questions and evaluating its performance. The goal is to see how well the trained BlueBERT model can extract accurate answers and how effectively FLAN-T5 simplifies them.\n",
        "\n",
        "The `find_relevant_context` function implements a simple but effective keyword matching algorithm to search through the dataset and find the most relevant context for a given question. It works by converting both the question and dataset entries to lowercase, then calculating a relevance score based on how many keywords match. I weighted question matches higher (score of 3) than answer matches (score of 1) because finding similar questions usually leads to better context. This approach helps the model work even when the exact question hasn't been seen during training.\n",
        "\n",
        "The `answer_question` function is the core of the Q&A system. It takes a question and optional context, tokenizes them together, and feeds them to the trained BlueBERT model. The model outputs start and end logits, which indicate where in the context the answer likely appears. By taking the argmax of these logits, the code identifies the most probable start and end positions, then extracts those tokens and decodes them back into readable text.\n",
        "\n",
        "I added confidence scoring by applying softmax to the logits, which gives a probability between 0 and 1 for how certain the model is about its answer. If the extracted answer seems too short or empty, the function falls back to using the first 500 characters of the context, ensuring there's always something meaningful to display. When the `simplify` parameter is True, the function calls the FLAN-T5 simplification function and includes both the original and simplified answers in the results.\n",
        "\n",
        "The testing section tries multiple data sources in order of preference: first attempting to upload a CSV file in Google Colab, then checking for local test files, and finally falling back to hardcoded sample questions if no external data is available. This flexibility ensures the code works in different environments. For each test case, it displays the question, ground truth answer (if available), the BlueBERT extracted answer with confidence scores, and the FLAN-T5 simplified version, making it easy to compare and evaluate the system's performance at a glance."
      ],
      "metadata": {
        "id": "6PF32nxf1fsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST THE Q&A SYSTEM WITH SAMPLE QUESTIONS FROM DATASET\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING Q&A SYSTEM WITH REAL CARDIOVASCULAR DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def find_relevant_context(question_text, dataset_df, top_k=1):\n",
        "    \"\"\"\n",
        "    Find the most relevant context from the dataset based on keyword matching\n",
        "    \"\"\"\n",
        "    question_lower = question_text.lower()\n",
        "\n",
        "    # Calculate relevance score for each row\n",
        "    scores = []\n",
        "    for idx, row in dataset_df.iterrows():\n",
        "        q = str(row.get('question', '')).lower()\n",
        "        a = str(row.get('answer', '')).lower()\n",
        "\n",
        "        # Simple keyword matching score\n",
        "        score = 0\n",
        "        # Check if question keywords appear in dataset question or answer\n",
        "        for word in question_lower.split():\n",
        "            if len(word) > 3:  # Only consider words longer than 3 chars\n",
        "                if word in q:\n",
        "                    score += 3  # Higher weight for question match\n",
        "                if word in a:\n",
        "                    score += 1  # Lower weight for answer match\n",
        "\n",
        "        scores.append((idx, score, str(row.get('answer', ''))))\n",
        "\n",
        "    # Sort by score and get top result\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if scores and scores[0][1] > 0:\n",
        "        return scores[0][2]  # Return the answer with highest score\n",
        "    else:\n",
        "        # Fallback to first answer if no good match\n",
        "        return str(dataset_df.iloc[0]['answer']) if len(dataset_df) > 0 else \"\"\n",
        "\n",
        "def answer_question(question_text, context=None, simplify=False):\n",
        "    \"\"\"\n",
        "    Answer a cardiovascular question using the trained model\n",
        "\n",
        "    Args:\n",
        "        question_text: The question to answer\n",
        "        context: Optional context (if None, finds relevant context from dataset)\n",
        "        simplify: Whether to simplify the answer using FLAN-T5\n",
        "\n",
        "    Returns:\n",
        "        dict with original_answer and simplified_answer (if simplify=True)\n",
        "    \"\"\"\n",
        "    # If no context provided, find relevant context from dataset\n",
        "    if context is None:\n",
        "        # Use intelligent keyword matching to find relevant context\n",
        "        context = find_relevant_context(question_text, dataset)\n",
        "\n",
        "    # Tokenize question and context for extractive QA\n",
        "    inputs = tokenizer(\n",
        "        question_text,\n",
        "        context,\n",
        "        truncation=\"only_second\",\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Ensure end_index is after start_index\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    # Extract answer from the context\n",
        "    answer_tokens = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
        "    original_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # If answer is too short or empty, use part of context\n",
        "    if len(original_answer.strip()) < 10:\n",
        "        original_answer = context[:2000]  # Use first 200 chars of context\n",
        "\n",
        "    # Calculate confidence (using softmax on logits)\n",
        "    start_probs = torch.softmax(start_logits, dim=1)\n",
        "    end_probs = torch.softmax(end_logits, dim=1)\n",
        "    confidence_start = start_probs[0, start_index].item()\n",
        "    confidence_end = end_probs[0, end_index].item()\n",
        "\n",
        "    result = {\n",
        "        \"question\": question_text,\n",
        "        \"original_answer\": original_answer,\n",
        "        \"confidence\": {\n",
        "            \"start\": confidence_start,\n",
        "            \"end\": confidence_end,\n",
        "            \"average\": (confidence_start + confidence_end) / 2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if simplify and original_answer.strip():\n",
        "        try:\n",
        "            simplified_answer_text = simplify_answer(original_answer)\n",
        "            result[\"simplified_answer\"] = simplified_answer_text\n",
        "        except Exception as e:\n",
        "            result[\"simplified_answer\"] = f\"Error simplifying answer: {e}\"\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"\\nAll tests completed!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkAxJJsglFtN",
        "outputId": "b1fe846b-ecb8-4c85-ac85-9f7c3d2edf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING Q&A SYSTEM WITH REAL CARDIOVASCULAR DATA\n",
            "============================================================\n",
            "\n",
            "All tests completed!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Q&A Interface with Widgets\n",
        "\n",
        "This code cell builds an interactive web-based interface specifically for Google Colab users, allowing anyone to ask cardiovascular health questions and receive immediate answers without writing any code themselves. The interface makes the Q&A system accessible to people who might not be familiar with programming.\n",
        "\n",
        "The implementation starts by importing Google Colab-specific libraries and IPython widgets, which provide the visual components. I created three main widgets: a large text area where users can type their questions, a checkbox to toggle answer simplification on or off, and a submit button that triggers the question-answering process. The layout parameters ensure everything is properly sized and visually organized within the Colab notebook interface.\n",
        "\n",
        "The `on_submit_clicked` function handles what happens when someone clicks the \"Get Answer\" button. It first validates that a question was actually entered, then uses the output widget to display a loading message while processing. The function calls `find_relevant_context` to search the dataset for appropriate context, then passes everything to the `answer_question` function. Results are displayed in a structured format showing the original question, the context that was found (with its character length), the BlueBERT answer with confidence percentage, and the FLAN-T5 simplified version if that option is enabled.\n",
        "\n",
        "I wrapped the entire widget creation in a try-except block because these widgets only work in Google Colab's environment. If the code runs in a different environment like a local Jupyter notebook or regular Python script, it gracefully handles the ImportError and instead prints instructions for using the `answer_question` function directly. The interface displays with a styled header and description, making it clear and inviting for users to interact with, effectively transforming the technical model into a user-friendly health information tool."
      ],
      "metadata": {
        "id": "l4_rcrZT1nQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERACTIVE WIDGET FOR GOOGLE COLAB\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CREATING INTERACTIVE Q&A INTERFACE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    from google.colab import output  # type: ignore\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import ipywidgets as widgets\n",
        "\n",
        "    # Create widgets\n",
        "    question_input = widgets.Textarea(\n",
        "        value='',\n",
        "        placeholder='Enter your cardiovascular health question here...',\n",
        "        description='Question:',\n",
        "        layout=widgets.Layout(width='100%', height='80px'),\n",
        "        style={'description_width': '100px'}\n",
        "    )\n",
        "\n",
        "    simplify_checkbox = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Simplify answer with FLAN-T5',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(\n",
        "        description='Get Answer',\n",
        "        button_style='primary',\n",
        "        tooltip='Click to get answer',\n",
        "        icon='check',\n",
        "        layout=widgets.Layout(width='150px', height='40px')\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_submit_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "\n",
        "            question = question_input.value.strip()\n",
        "            if not question:\n",
        "                print(\"Please enter a question!\")\n",
        "                return\n",
        "\n",
        "            print(\"Processing your question...\")\n",
        "            print(\"Searching for relevant context in dataset...\\n\")\n",
        "\n",
        "            # Find relevant context first\n",
        "            relevant_context = find_relevant_context(question, dataset)\n",
        "\n",
        "            # Get answer with the relevant context\n",
        "            result = answer_question(\n",
        "                question_text=question,\n",
        "                context=relevant_context,\n",
        "                simplify=simplify_checkbox.value\n",
        "            )\n",
        "\n",
        "            # Display results\n",
        "            print(\"=\" * 60)\n",
        "            print(\"QUESTION\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"{result['question']}\\n\")\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "            print(\"FOUND CONTEXT\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"{relevant_context[:2500]}...\\n\")\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "            print(\"ORIGINAL ANSWER (BlueBERT)\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"{result['original_answer']}\\n\")\n",
        "\n",
        "            print(f\"Confidence: {result['confidence']['average']:.2%}\\n\")\n",
        "\n",
        "            if 'simplified_answer' in result:\n",
        "                print(\"=\" * 60)\n",
        "                print(\"SIMPLIFIED ANSWER (FLAN-T5)\")\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"{result['simplified_answer']}\\n\")\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "    submit_button.on_click(on_submit_clicked)\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(\"<h2 style='color: #1976D2;'>Cardiovascular Health Q&A System</h2>\"))\n",
        "    display(HTML(\"<p style='color: #666;'>Ask any cardiovascular health question and get answers from our AI model!</p>\"))\n",
        "    display(HTML(\"<p style='color: #999; font-size: 0.9em;'> The system intelligently searches the dataset for relevant medical context to answer your question.</p>\"))\n",
        "\n",
        "    display(question_input)\n",
        "    display(simplify_checkbox)\n",
        "    display(submit_button)\n",
        "    display(output_area)\n",
        "\n",
        "    print(\"\\nInteractive interface ready! Enter your question above.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Interactive widgets not available (not in Colab environment)\")\n",
        "    print(\"You can still use the answer_question() function directly:\")\n",
        "    print(\"\\nExample:\")\n",
        "    print('# Find relevant context')\n",
        "    print('context = find_relevant_context(\"What causes stroke?\", dataset)')\n",
        "    print('result = answer_question(\"What causes stroke?\", context=context, simplify=True)')\n",
        "    print('print(result[\"simplified_answer\"])')\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409,
          "referenced_widgets": [
            "0af66a22ccdb44bdb0b60e99941ca3c1",
            "2ef662430f254154ae2428c5ceebef2e",
            "b7b6bb1a60ba4623b4c44cf2c742bed6",
            "27f330795e354c84a78ba2955d710564",
            "9477effb4544414a9780e6dd375f0fe1",
            "08563c8c8f1d4f329d07de755a88901e",
            "a25d3835bf8e462986ba62ed1af7ce04",
            "2eab4395213f4fd796d8a4994f2df19e",
            "cc5248289e324300ac056f966b15011a",
            "4f2bcc8723284e6086cf85c96c9e2a79",
            "15f26723f76546c29a867c92e953348d"
          ]
        },
        "id": "8edpLgL1oSv1",
        "outputId": "ef03283a-03f1-431d-94ec-392fbc475cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING INTERACTIVE Q&A INTERFACE\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2 style='color: #1976D2;'>Cardiovascular Health Q&A System</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style='color: #666;'>Ask any cardiovascular health question and get answers from our AI model!</p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style='color: #999; font-size: 0.9em;'> The system intelligently searches the dataset for relevant medical context to answer your question.</p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Question:', layout=Layout(height='80px', width='100%'), placeholder='Enter you…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0af66a22ccdb44bdb0b60e99941ca3c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=True, description='Simplify answer with FLAN-T5', style=DescriptionStyle(description_width='ini…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27f330795e354c84a78ba2955d710564"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='primary', description='Get Answer', icon='check', layout=Layout(height='40px', width='150…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a25d3835bf8e462986ba62ed1af7ce04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f2bcc8723284e6086cf85c96c9e2a79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Interactive interface ready! Enter your question above.\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}